"""
AST ê¸°ë°˜ ìŒì› ë¶„ë¦¬ íŒŒì´í”„ë¼ì¸ - ìµœì¢… í†µí•© ë²„ì „
í…œí”Œë¦¿ ê¸°ë°˜ ë¶„í¬ ìœ ì‚¬ë„ë¥¼ í™œìš©í•œ ì •êµí•œ ìŒì› ë¶„ë¦¬
"""

import numpy as np
import torch
import librosa
import soundfile as sf
import warnings
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass
import matplotlib.pyplot as plt
from matplotlib.patches import Rectangle
from transformers import ASTFeatureExtractor, ASTForAudioClassification
from scipy import signal
from scipy.ndimage import gaussian_filter1d, uniform_filter1d
from scipy.stats import wasserstein_distance
import json
import argparse
import os
import sys

# í™˜ê²½ ì„¤ì •
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'
warnings.filterwarnings("ignore")

# ==================== ìƒìˆ˜ ì •ì˜ ====================
class AudioConfig:
    """ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì„¤ì •"""
    SR = 16000  # ìƒ˜í”Œë§ ë ˆì´íŠ¸
    INPUT_SEC = 4.096  # ì‹¤ì œ ì…ë ¥ ì˜¤ë””ì˜¤ ê¸¸ì´
    MODEL_SEC = 10.24  # AST ëª¨ë¸ ì²˜ë¦¬ ê¸¸ì´
    L_INPUT = int(round(INPUT_SEC * SR))  # 65,536 ìƒ˜í”Œ
    L_MODEL = int(round(MODEL_SEC * SR))  # 163,840 ìƒ˜í”Œ

class STFTConfig:
    """STFT ì„¤ì •"""
    N_FFT = 400
    HOP = 160
    WINLEN = 400
    N_MELS = 128

class ASTConfig:
    """AST ëª¨ë¸ ì„¤ì •"""
    MAX_LENGTH = 1024
    FREQ_PATCHES = 12  # (128 - 16) / 10 + 1
    TIME_PATCHES = 101  # (1024 - 16) / 10 + 1

class SeparationConfig:
    """ë¶„ë¦¬ íŒŒë¼ë¯¸í„°"""
    SIMILARITY_THRESHOLD = 0.6  # 0.6 â†’ 0.4ë¡œ ë‚®ì¶¤ (ë” ë§ì€ êµ¬ê°„ ë¶„ë¦¬)
    WEAK_MASK_FACTOR = 0.1  # 0.1 â†’ 0.3ìœ¼ë¡œ ì¦ê°€ (ì•½í•œ êµ¬ê°„ë„ ë” ê°•í•˜ê²Œ)
    MIN_ANCHOR_FRAMES = 8  # ìµœì†Œ ì•µì»¤ í¬ê¸° (0.08ì´ˆ)
    MAX_ANCHOR_FRAMES = 128  # ìµœëŒ€ ì•µì»¤ í¬ê¸° (1.28ì´ˆ)
    CONTINUITY_GAP = 1  # ì—°ì†ì„± íŒë‹¨ ìµœëŒ€ ê°„ê²© (íŒ¨ì¹˜)
    ATTENTION_PERCENTILE = 80  # ìƒìœ„ ì–´í…ì…˜ í¼ì„¼íƒ€ì¼
    MEDIUM_MASK_FACTOR = 0.6  # ì¤‘ê°„ ìœ ì‚¬ë„ êµ¬ê°„ ë§ˆìŠ¤í¬ ê°•ë„

class WienerConfig:
    """Wiener í•„í„°ë§ ì„¤ì •"""
    TANH_SCALE = 3.0  # tanh í•¨ìˆ˜ ìŠ¤ì¼€ì¼ë§ (2.0 â†’ 3.0)
    SIGMOID_SCALE = 10.0  # sigmoid í•¨ìˆ˜ ìŠ¤ì¼€ì¼ë§ (8.0 â†’ 10.0)
    SIGMOID_THRESHOLD = 0.6  # sigmoid í™œì„±í™” ì„ê³„ê°’ (0.7 â†’ 0.6)
    ANCHOR_BOOST = 1.5  # ì•µì»¤ êµ¬ê°„ ë¶€ìŠ¤íŠ¸ (1.2 â†’ 1.5)

# í¸ì˜ë¥¼ ìœ„í•œ ë³„ì¹­
SR = AudioConfig.SR
INPUT_SEC = AudioConfig.INPUT_SEC
MODEL_SEC = AudioConfig.MODEL_SEC
L_INPUT = AudioConfig.L_INPUT
L_MODEL = AudioConfig.L_MODEL
N_FFT = STFTConfig.N_FFT
HOP = STFTConfig.HOP
WINLEN = STFTConfig.WINLEN
N_MELS = STFTConfig.N_MELS
AST_FREQ_PATCHES = ASTConfig.FREQ_PATCHES
AST_TIME_PATCHES = ASTConfig.TIME_PATCHES
SIMILARITY_THRESHOLD = SeparationConfig.SIMILARITY_THRESHOLD

# ==================== ë°ì´í„° í´ë˜ìŠ¤ ====================
@dataclass
class SeparationResult:
    """ë¶„ë¦¬ ê²°ê³¼ ì €ì¥"""
    separated_audio: np.ndarray
    residual_audio: np.ndarray
    mask: np.ndarray
    anchor_frames: Tuple[int, int]
    energy_ratio: float
    classification: Dict[str, Any]
    attention_matrix: np.ndarray
    template_similarity: np.ndarray

# ==================== AST ëª¨ë¸ ì²˜ë¦¬ ====================
class ASTProcessor:
    """AST ëª¨ë¸ ì²˜ë¦¬ - CPU ì „ìš©"""
    
    def __init__(self, model_name: str = "MIT/ast-finetuned-audioset-10-10-0.4593"):
        # CPU ê°•ì œ ì„¤ì •
        self.device = torch.device("cpu")
        torch.set_num_threads(4)
        print(f"ë””ë°”ì´ìŠ¤: CPU (ìŠ¤ë ˆë“œ: 4)")
        
        # ëª¨ë¸ ë¡œë“œ
        self.feature_extractor = ASTFeatureExtractor.from_pretrained(model_name)
        
        try:
            self.model = ASTForAudioClassification.from_pretrained(
                model_name,
                attn_implementation="eager",
                output_attentions=True,
                output_hidden_states=True
            )
        except:
            self.model = ASTForAudioClassification.from_pretrained(
                model_name,
                output_attentions=True,
                output_hidden_states=True
            )
        
        # CPU ìµœì í™”
        try:
            self.model = torch.quantization.quantize_dynamic(
                self.model, {torch.nn.Linear}, dtype=torch.qint8
            )
            print("ì–‘ìí™” ì ìš©: ì„±ê³µ")
        except:
            print("ì–‘ìí™” ì ìš©: ì‹¤íŒ¨ (ì¼ë°˜ ëª¨ë“œ)")
        
        self.model.eval()
        print("AST ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    
    def process(self, audio: np.ndarray) -> Tuple[np.ndarray, Dict, np.ndarray]:
        """10.24ì´ˆë¡œ íŒ¨ë”©í•˜ì—¬ ì²˜ë¦¬"""
        # íŒ¨ë”©
        padded = np.pad(audio, (0, max(0, L_MODEL - len(audio))), mode='constant')[:L_MODEL]
        
        # íŠ¹ì§• ì¶”ì¶œ
        inputs = self.feature_extractor(padded, sampling_rate=SR, return_tensors="pt")
        
        # ì¶”ë¡ 
        with torch.no_grad():
            outputs = self.model(**inputs)
        
        # ì–´í…ì…˜ ì¶”ì¶œ
        if outputs.attentions:
            attention = outputs.attentions[-1].squeeze(0).mean(dim=0)
            attention_to_patches = attention[0, 1:]
        else:
            attention_to_patches = torch.ones(AST_FREQ_PATCHES * AST_TIME_PATCHES) / (AST_FREQ_PATCHES * AST_TIME_PATCHES)
        
        attention_matrix = attention_to_patches[:AST_FREQ_PATCHES * AST_TIME_PATCHES].reshape(
            AST_FREQ_PATCHES, AST_TIME_PATCHES
        ).cpu().numpy()
        
        # CLS/Distill í† í° ì˜í–¥ ì œê±°: ì¢Œìƒë‹¨ ë ë¶€ë¶„ ë§ˆìŠ¤í‚¹
        # ì²« ë²ˆì§¸ ì£¼íŒŒìˆ˜ íŒ¨ì¹˜ì™€ ì²« ë²ˆì§¸ ì‹œê°„ íŒ¨ì¹˜ì˜ ì–´í…ì…˜ì„ ë‚®ì¶¤
        attention_matrix[0, 0] *= 0.1  # ì¢Œìƒë‹¨ ë ë¶€ë¶„ ê°•ë ¥íˆ ì–µì œ
        if attention_matrix.shape[0] > 1:
            attention_matrix[1, 0] *= 0.3  # ë‘ ë²ˆì§¸ ì£¼íŒŒìˆ˜ íŒ¨ì¹˜ë„ ë¶€ë¶„ì ìœ¼ë¡œ ì–µì œ
        if attention_matrix.shape[1] > 1:
            attention_matrix[0, 1] *= 0.3  # ë‘ ë²ˆì§¸ ì‹œê°„ íŒ¨ì¹˜ë„ ë¶€ë¶„ì ìœ¼ë¡œ ì–µì œ
        
        print(f"    ğŸ” CLS/distillation token masking applied to attention matrix")
        
        # ë¶„ë¥˜ ê²°ê³¼
        logits = outputs.logits
        probs = torch.softmax(logits, dim=-1)
        top5_probs, top5_indices = torch.topk(probs, k=min(5, probs.shape[-1]), dim=-1)
        
        classification = {
            'predicted_class': self.model.config.id2label.get(top5_indices[0, 0].item(), "unknown"),
            'confidence': top5_probs[0, 0].item(),
            'top5_classes': [self.model.config.id2label.get(idx.item(), f"class_{idx}") 
                           for idx in top5_indices[0]],
            'top5_probs': top5_probs[0].cpu().numpy().tolist()
        }
        
        spectrogram = inputs['input_values'].squeeze(0).cpu().numpy()
        
        return attention_matrix, classification, spectrogram

# ==================== ì˜¤ë””ì˜¤ ì²˜ë¦¬ í•¨ìˆ˜ ====================
def load_audio(file_path: str) -> Tuple[np.ndarray, np.ndarray]:
    """ì˜¤ë””ì˜¤ ë¡œë“œ ë° íŒ¨ë”©"""
    audio, _ = librosa.load(file_path, sr=SR, mono=True)
    
    # 4.096ì´ˆ ë²„ì „
    audio_4sec = audio[:L_INPUT] if len(audio) > L_INPUT else np.pad(audio, (0, L_INPUT - len(audio)))
    
    # 10.24ì´ˆ ë²„ì „
    audio_10sec = audio[:L_MODEL] if len(audio) > L_MODEL else np.pad(audio, (0, L_MODEL - len(audio)))
    
    return audio_4sec, audio_10sec

def compute_stft(audio: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """STFT ê³„ì‚°"""
    stft = librosa.stft(audio, n_fft=N_FFT, hop_length=HOP, win_length=WINLEN)
    mag = np.abs(stft)
    phase = np.angle(stft)
    return stft, mag, phase

# ==================== í…œí”Œë¦¿ ê¸°ë°˜ ë¶„í¬ ìœ ì‚¬ë„ ====================
def calculate_distribution_similarity(template: np.ndarray, current: np.ndarray) -> float:
    """ì£¼íŒŒìˆ˜-ì§„í­ ë¶„í¬ ìœ ì‚¬ë„ ê³„ì‚° - ëª¨ë“  ê°’ ì–‘ìˆ˜ ë³´ì¥"""
    
    # 1. ìŒìˆ˜ ê°’ ì œê±° ë° ìµœì†Œê°’ ë³´ì¥ (ì–‘ìˆ˜ ë²”ìœ„ë¡œ ë³€í™˜)
    template_positive = np.maximum(template, 1e-8)  # ìŒìˆ˜ ì œê±°, ìµœì†Œê°’ ë³´ì¥
    current_positive = np.maximum(current, 1e-8)    # ìŒìˆ˜ ì œê±°, ìµœì†Œê°’ ë³´ì¥
    
    # 2. ì •ê·œí™” (í™•ë¥  ë¶„í¬ë¡œ ë³€í™˜)
    template_norm = template_positive / (np.sum(template_positive) + 1e-8)
    current_norm = current_positive / (np.sum(current_positive) + 1e-8)
    
    # 3. ì •ê·œí™”ëœ ê°’ë„ ì–‘ìˆ˜ ë³´ì¥
    template_norm = np.maximum(template_norm, 1e-8)
    current_norm = np.maximum(current_norm, 1e-8)
    
    # 4. ì½”ì‚¬ì¸ ìœ ì‚¬ë„
    cosine_sim = np.dot(template_norm, current_norm) / (
        np.linalg.norm(template_norm) * np.linalg.norm(current_norm) + 1e-8
    )
    cosine_sim = np.clip(cosine_sim, 0, 1)  # [0, 1] ë²”ìœ„ ë³´ì¥
    
    # 5. Earth Mover's Distance (ì–‘ìˆ˜ ê°’ë§Œ ì‚¬ìš©)
    try:
        emd = wasserstein_distance(np.arange(len(template_norm)), 
                                   np.arange(len(current_norm)),
                                   template_norm, current_norm)
        emd_sim = np.exp(-emd / 10)
        emd_sim = np.clip(emd_sim, 0, 1)  # [0, 1] ë²”ìœ„ ë³´ì¥
    except Exception as e:
        print(f"    âš ï¸ EMD calculation failed: {e}, using fallback")
        # EMD ì‹¤íŒ¨ ì‹œ ëŒ€ì•ˆ: L1 ê±°ë¦¬ ê¸°ë°˜ ìœ ì‚¬ë„
        l1_distance = np.sum(np.abs(template_norm - current_norm))
        emd_sim = np.exp(-l1_distance)
        emd_sim = np.clip(emd_sim, 0, 1)
    
    # 6. ìŠ¤í™íŠ¸ëŸ´ ì¤‘ì‹¬ ë¹„êµ
    freqs = np.arange(len(template))
    template_centroid = np.sum(freqs * template_norm)
    current_centroid = np.sum(freqs * current_norm)
    centroid_diff = abs(template_centroid - current_centroid) / len(template)
    centroid_sim = np.exp(-centroid_diff * 5)
    centroid_sim = np.clip(centroid_sim, 0, 1)  # [0, 1] ë²”ìœ„ ë³´ì¥
    
    # 7. ì¢…í•© ìœ ì‚¬ë„ (ëª¨ë“  ê°’ì´ [0, 1] ë²”ìœ„)
    total_sim = 0.5 * cosine_sim + 0.3 * emd_sim + 0.2 * centroid_sim
    total_sim = np.clip(total_sim, 0, 1)  # ìµœì¢… [0, 1] ë²”ìœ„ ë³´ì¥
    
    return total_sim

# ==================== í—¬í¼ í•¨ìˆ˜ë“¤ ====================
def select_best_patch(stft_mag: np.ndarray, anchor_start: int, anchor_end: int, 
                     patch_size: int, num_patches: int) -> Tuple[np.ndarray, int, int]:
    """ì•µì»¤ êµ¬ê°„ì—ì„œ ê°€ì¥ ì—ë„ˆì§€ê°€ ë†’ì€ íŒ¨ì¹˜ ì„ íƒ"""
    patch_energies = []
    for i in range(num_patches):
        patch_start = anchor_start + i * patch_size
        patch_end = min(anchor_start + (i + 1) * patch_size, anchor_end)
        patch_spec = stft_mag[:, patch_start:patch_end]
        patch_energy = np.sum(patch_spec ** 2)
        patch_energies.append(patch_energy)
    
    # ê°€ì¥ ì—ë„ˆì§€ê°€ ë†’ì€ íŒ¨ì¹˜ ì„ íƒ
    best_patch_idx = np.argmax(patch_energies)
    best_patch_start = anchor_start + best_patch_idx * patch_size
    best_patch_end = min(anchor_start + (best_patch_idx + 1) * patch_size, anchor_end)
    best_patch_spec = stft_mag[:, best_patch_start:best_patch_end]
    
    print(f"    ğŸ” Selected best patch {best_patch_idx}/{num_patches} (energy: {patch_energies[best_patch_idx]:.3f})")
    
    return best_patch_spec, best_patch_start, best_patch_end

def calculate_patch_size(anchor_size: int) -> int:
    """ì•µì»¤ í¬ê¸°ì— ë”°ë¥¸ ì ì‘ì  íŒ¨ì¹˜ í¬ê¸° ê²°ì • - ìµœì†Œ ì‹œê°„ êµ¬ê°„ìœ¼ë¡œ ì„¤ì •"""
    # ì—°ì†ì ì¸ ì‹œê°„ íŒ¨í„´ì„ í™•ì¸í•  ìˆ˜ ìˆëŠ” ìµœì†Œ ì‹œê°„ êµ¬ê°„
    min_frames = 4  # ìµœì†Œ 4 í”„ë ˆì„ (0.04ì´ˆ) - ë§¤ìš° ì§§ì€ êµ¬ê°„
    max_frames = 8  # ìµœëŒ€ 8 í”„ë ˆì„ (0.08ì´ˆ) - ì§§ì€ êµ¬ê°„
    
    if anchor_size <= min_frames:
        return anchor_size  # ì „ì²´ ì‚¬ìš©
    elif anchor_size <= max_frames:
        return min_frames  # ìµœì†Œ êµ¬ê°„ ì‚¬ìš©
    else:
        return min_frames  # í•­ìƒ ìµœì†Œ êµ¬ê°„ ì‚¬ìš© (ì‹œê°„ íŒ¨í„´ í™•ì¸ìš©)

def apply_smoothing_filters(signal: np.ndarray, is_2d: bool = False) -> np.ndarray:
    """ì‹ í˜¸ì— ë‹¤ë‹¨ê³„ ìŠ¤ë¬´ë”© í•„í„° ì ìš©"""
    if is_2d:
        # 2D ì‹ í˜¸ (ë§ˆìŠ¤í¬)
        # 1ë‹¨ê³„: ì£¼íŒŒìˆ˜ ì¶• ë°©í–¥ ìŠ¤ë¬´ë”©
        signal = gaussian_filter1d(signal, sigma=1.5, axis=0)
        # 2ë‹¨ê³„: ì‹œê°„ì¶• ë°©í–¥ ìŠ¤ë¬´ë”©
        signal = gaussian_filter1d(signal, sigma=1.0, axis=1)
        # 3ë‹¨ê³„: 2D ê°€ìš°ì‹œì•ˆ í•„í„°
        from scipy.ndimage import gaussian_filter
        signal = gaussian_filter(signal, sigma=(0.8, 0.6))
    else:
        # 1D ì‹ í˜¸ (ìœ ì‚¬ë„)
        from scipy.ndimage import median_filter
        signal = median_filter(signal, size=5)  # ì¤‘ì•™ê°’ í•„í„°
        signal = uniform_filter1d(signal, size=3, mode='nearest')  # í‰ê·  í•„í„°
    
    return np.clip(signal, 0, 1)

def validate_anchor_bounds(anchor_start: int, anchor_end: int, max_frames: int) -> Tuple[int, int]:
    """ì•µì»¤ ê²½ê³„ ê²€ì¦ ë° ì¡°ì •"""
    anchor_start = max(0, min(anchor_start, max_frames - 1))
    anchor_end = max(anchor_start + 1, min(anchor_end, max_frames))
    return anchor_start, anchor_end

# ==================== ì•µì»¤ ì„ ì • ====================
def find_attention_anchor(
    attention_matrix: np.ndarray,
    suppressed_regions: List[Tuple[int, int]],
    previous_peaks: List[Tuple[int, int]],
    audio_length: int
) -> Tuple[int, int, bool]:
    """ì™„ì „íˆ ìœ ë™ì ì¸ ì•µì»¤ ì„ ì • - ì–´í…ì…˜ ë§¤íŠ¸ë¦­ìŠ¤ì—ì„œ ì—°ì†ì ì¸ ë¶€ë¶„ë§Œ"""
    num_freq_patches, num_time_patches = attention_matrix.shape
    max_frames = audio_length // HOP
    valid_patches = int(INPUT_SEC / MODEL_SEC * num_time_patches)
    
    print(f"    ğŸ” Anchor selection: {num_freq_patches}x{num_time_patches} attention matrix")
    print(f"    ğŸ” Valid patches for 4.096s: {valid_patches}/{num_time_patches}")
    
    # 1. ì–µì œëœ ì˜ì—­ ë§ˆìŠ¤í‚¹
    att_masked = attention_matrix.copy()
    for start, end in suppressed_regions:
        start_patch = int(start * num_time_patches / (L_MODEL // HOP))
        end_patch = int(end * num_time_patches / (L_MODEL // HOP))
        att_masked[:, max(0, start_patch):min(num_time_patches, end_patch)] *= 0.05
        print(f"    ğŸ” Suppressed region: patches {start_patch}-{end_patch}")
    
    # 2. ì´ì „ íŒ¨ìŠ¤ì˜ ìƒìœ„ ì–´í…ì…˜ ì˜ì—­ ì•½í™” (ë” ê´€ëŒ€í•˜ê²Œ)
    if previous_peaks:
        for start, end in previous_peaks:
            # 0.3 â†’ 0.5ë¡œ ë” ì™„í™” (ì—°ì† êµ¬ê°„ì„ ì°¾ì„ ìˆ˜ ìˆë„ë¡)
            att_masked[:, max(0, start):min(num_time_patches, end)] *= 0.5
            print(f"    ğŸ” Weakened previous peak: patches {start}-{end} (factor: 0.5)")
    
    # 3. 4.096ì´ˆ ë²”ìœ„ ë‚´ì—ì„œë§Œ íƒìƒ‰
    att_masked[:, valid_patches:] = 0
    
    # 4. ì£¼íŒŒìˆ˜ ì˜ì—­ë³„ ì—°ì†ì ì¸ í™œì„±í™” êµ¬ê°„ ì°¾ê¸°
    print(f"    ğŸ” Finding continuous frequency-time attention regions...")
    
    # ê° ì£¼íŒŒìˆ˜ë³„ë¡œ ë†’ì€ ì–´í…ì…˜ ì˜ì—­ ì‹ë³„
    num_freq_patches = att_masked.shape[0]
    attention_threshold = np.percentile(att_masked, SeparationConfig.ATTENTION_PERCENTILE)
    
    print(f"    ğŸ” Attention threshold: {attention_threshold:.4f}")
    
    # ì£¼íŒŒìˆ˜ë³„ í™œì„±í™” ë§ˆìŠ¤í¬ ìƒì„±
    freq_activation_maps = []
    for freq_idx in range(num_freq_patches):
        freq_attention = att_masked[freq_idx, :valid_patches]
        high_attention_mask = freq_attention >= attention_threshold
        freq_activation_maps.append(high_attention_mask)
    
    # 5. ì£¼íŒŒìˆ˜ ì˜ì—­ë³„ ì—°ì† êµ¬ê°„ ì°¾ê¸°
    frequency_segments = []
    
    for freq_idx, activation_map in enumerate(freq_activation_maps):
        if not np.any(activation_map):
            continue
            
        # í•´ë‹¹ ì£¼íŒŒìˆ˜ì—ì„œ ì—°ì†ì ì¸ ì‹œê°„ êµ¬ê°„ ì°¾ê¸°
        high_indices = np.where(activation_map)[0]
        
        # ì—°ì† êµ¬ê°„ ê·¸ë£¹í™”
        continuous_segments = []
        current_segment = [high_indices[0]]
        
        for i in range(1, len(high_indices)):
            if high_indices[i] - high_indices[i-1] <= SeparationConfig.CONTINUITY_GAP:
                current_segment.append(high_indices[i])
            else:
                if len(current_segment) >= 2:  # ìµœì†Œ 2 íŒ¨ì¹˜ ì´ìƒ
                    continuous_segments.append((freq_idx, current_segment))
                current_segment = [high_indices[i]]
        
        # ë§ˆì§€ë§‰ êµ¬ê°„ ì¶”ê°€
        if len(current_segment) >= 2:
            continuous_segments.append((freq_idx, current_segment))
        
        frequency_segments.extend(continuous_segments)
    
    print(f"    ğŸ” Found {len(frequency_segments)} frequency-time segments")
    
    # 6. ì£¼íŒŒìˆ˜ ì˜ì—­ë³„ ì—°ì†ì„± ì ìˆ˜ ê³„ì‚°
    best_segment = None
    best_freq_idx = None
    best_score = -1
    
    for freq_idx, segment in frequency_segments:
        # í•´ë‹¹ ì£¼íŒŒìˆ˜-ì‹œê°„ êµ¬ê°„ì˜ í‰ê·  ì–´í…ì…˜
        segment_attention = np.mean([att_masked[freq_idx, i] for i in segment])
        
        # ì ìˆ˜ = ê¸¸ì´ Ã— í‰ê·  ì–´í…ì…˜ Ã— ì£¼íŒŒìˆ˜ ê°€ì¤‘ì¹˜
        # ì €ì£¼íŒŒ(0-3)ì™€ ê³ ì£¼íŒŒ(8-11)ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜
        freq_weight = 1.0
        if freq_idx <= 3:  # ì €ì£¼íŒŒ
            freq_weight = 1.2
        elif freq_idx >= 8:  # ê³ ì£¼íŒŒ
            freq_weight = 1.1
        
        score = len(segment) * segment_attention * freq_weight
        
        print(f"    ğŸ” Freq {freq_idx}, Time {segment[0]}-{segment[-1]}: length={len(segment)}, attention={segment_attention:.4f}, score={score:.4f}")
        
        if score > best_score:
            best_score = score
            best_segment = segment
            best_freq_idx = freq_idx
    
    if best_segment is None:
        print(f"    âš ï¸ No valid continuous segment found, using max attention fallback")
        # í´ë°±: ì „ì²´ ì–´í…ì…˜ ë§¤íŠ¸ë¦­ìŠ¤ì—ì„œ ìµœëŒ€ ì–´í…ì…˜ ìœ„ì¹˜ ì°¾ê¸°
        max_freq_idx, max_time_idx = np.unravel_index(np.argmax(att_masked), att_masked.shape)
        # ìµœëŒ€ ì–´í…ì…˜ ì£¼ë³€ 3-5 íŒ¨ì¹˜ë¥¼ ì•µì»¤ë¡œ ì„¤ì •
        anchor_size = min(5, valid_patches // 8)  # ìµœëŒ€ 5íŒ¨ì¹˜ ë˜ëŠ” ì „ì²´ì˜ 1/8
        start_patch = max(0, max_time_idx - anchor_size // 2)
        end_patch = min(valid_patches, start_patch + anchor_size)
        best_segment = list(range(start_patch, end_patch))
        best_freq_idx = max_freq_idx  # í´ë°±ì—ì„œë„ ì£¼íŒŒìˆ˜ ì •ë³´ ë³´ì¡´
        print(f"    ğŸ” Fallback anchor: patches {start_patch}-{end_patch-1} (max attention at freq {max_freq_idx}, time {max_time_idx})")
    
    # 7. ì•µì»¤ êµ¬ê°„ì„ ì—°ì†ì ì¸ ì–´í…ì…˜ ê¸¸ì´ì— ë§ì¶° ì„¤ì • (ì™„ì „ ìœ ë™ì )
    anchor_start_patch = best_segment[0]
    anchor_end_patch = best_segment[-1] + 1  # +1ë¡œ íŒ¨ì¹˜ ëê¹Œì§€ í¬í•¨
    
    # íŒ¨ì¹˜ë¥¼ í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
    anchor_start = int(anchor_start_patch * max_frames / valid_patches)
    anchor_end = int(anchor_end_patch * max_frames / valid_patches)
    
    # ê²½ê³„ ê²€ì¦
    anchor_start, anchor_end = validate_anchor_bounds(anchor_start, anchor_end, max_frames)
    
    actual_anchor_size = anchor_end - anchor_start
    actual_anchor_time = actual_anchor_size * HOP / SR
    
    print(f"    ğŸ” Selected continuous segment: {len(best_segment)} patches")
    print(f"    ğŸ” Dynamic anchor: {actual_anchor_size} frames ({actual_anchor_time:.3f}s)")
    print(f"    ğŸ” Anchor range: {anchor_start}-{anchor_end} frames")
    print(f"    ğŸ” Time range: {anchor_start*HOP/SR:.3f}s - {anchor_end*HOP/SR:.3f}s")
    
    # 8. í¬ê¸° ì œí•œ ì ìš©
    min_frames = SeparationConfig.MIN_ANCHOR_FRAMES
    max_frames_limit = SeparationConfig.MAX_ANCHOR_FRAMES
    
    if actual_anchor_size < min_frames:
        center = (anchor_start + anchor_end) // 2
        anchor_start = max(0, center - min_frames // 2)
        anchor_end = min(max_frames, anchor_start + min_frames)
        print(f"    âš ï¸ Expanded to minimum size: {anchor_end - anchor_start} frames")
    elif actual_anchor_size > max_frames_limit:
        center = (anchor_start + anchor_end) // 2
        anchor_start = max(0, center - max_frames_limit // 2)
        anchor_end = min(max_frames, anchor_start + max_frames_limit)
        print(f"    âš ï¸ Limited to maximum size: {anchor_end - anchor_start} frames")
    
    return anchor_start, anchor_end, True, best_freq_idx

# ==================== í…œí”Œë¦¿ ê¸°ë°˜ ë§ˆìŠ¤í‚¹ ====================
def create_template_mask(
    stft_mag: np.ndarray,
    anchor_frames: Tuple[int, int],
    attention_weights: Optional[np.ndarray] = None
) -> Tuple[np.ndarray, np.ndarray]:
    """í…œí”Œë¦¿ ê¸°ë°˜ ë¶„í¬ ìœ ì‚¬ë„ ë§ˆìŠ¤í‚¹ - ìˆ˜ì • ë²„ì „"""
    anchor_start, anchor_end = anchor_frames
    
    # ì•µì»¤ ë²”ìœ„ ê²€ì¦
    anchor_start = min(anchor_start, stft_mag.shape[1] - 1)
    anchor_end = min(anchor_end, stft_mag.shape[1])
    
    if anchor_start >= anchor_end:
        return np.zeros_like(stft_mag), np.zeros(stft_mag.shape[1])
    
    # === ìˆ˜ì • 1: ìœ ë™ì  ì•µì»¤ í¬ê¸°ì— ë§ì¶˜ í…œí”Œë¦¿ ìƒì„± ===
    anchor_spec = stft_mag[:, anchor_start:anchor_end]
    actual_anchor_size = anchor_end - anchor_start
    
    print(f"    ğŸ” Using dynamic anchor region: {actual_anchor_size} frames ({actual_anchor_size * HOP / SR:.3f}s)")
    
    # ì•µì»¤ í¬ê¸°ì— ë”°ë¼ ì ì‘ì  íŒ¨ì¹˜ í¬ê¸° ê²°ì •
    patch_size = calculate_patch_size(actual_anchor_size)
    
    if patch_size == actual_anchor_size:
        # ì‘ì€ ì•µì»¤: ì „ì²´ ì‚¬ìš©
        best_patch_spec = anchor_spec
        best_patch_start = anchor_start
        best_patch_end = anchor_end
        print(f"    ğŸ” Small anchor: using entire region ({actual_anchor_size} frames)")
    else:
        # ì¤‘ê°„/í° ì•µì»¤: íŒ¨ì¹˜ë¡œ ë‚˜ëˆ„ê¸°
        num_patches = max(1, actual_anchor_size // patch_size)
        best_patch_spec, best_patch_start, best_patch_end = select_best_patch(
            stft_mag, anchor_start, anchor_end, patch_size, num_patches
        )
        print(f"    ğŸ” Anchor: selected best {patch_size}-frame patch from {num_patches} patches")
    
    # === ìˆ˜ì • 2: ì–´í…ì…˜ ë§¤íŠ¸ë¦­ìŠ¤ì™€ ì§„í­ ê°•ì¡°ëœ ì£¼íŒŒìˆ˜ ì´ìš©í•œ í…œí”Œë¦¿ ìƒì„± ===
    
    # 1. ê¸°ë³¸ í…œí”Œë¦¿ (ìœ ë™ì  í¬ê¸° íŒ¨ì¹˜ì˜ í‰ê· )
    base_template = np.mean(best_patch_spec, axis=1)
    actual_patch_size = best_patch_spec.shape[1]
    print(f"    ğŸ” Template from {actual_patch_size}-frame patch ({actual_patch_size * HOP / SR:.3f}s)")
    
    # 2. ì–´í…ì…˜ ë§¤íŠ¸ë¦­ìŠ¤ì—ì„œ í•´ë‹¹ êµ¬ê°„ì˜ ì£¼íŒŒìˆ˜ ê°€ì¤‘ì¹˜ ì¶”ì¶œ (ê°œì„ )
    attention_weights = np.ones(len(base_template))  # ê¸°ë³¸ê°’
    if attention_weights is not None and len(attention_weights.shape) > 1:
        # ì•µì»¤ êµ¬ê°„ì— í•´ë‹¹í•˜ëŠ” ì–´í…ì…˜ íŒ¨ì¹˜ë“¤
        num_time_patches = attention_weights.shape[1]
        patch_start_attention = int(best_patch_start * num_time_patches / stft_mag.shape[1])
        patch_end_attention = int(best_patch_end * num_time_patches / stft_mag.shape[1])
        patch_start_attention = max(0, min(num_time_patches-1, patch_start_attention))
        patch_end_attention = max(0, min(num_time_patches, patch_end_attention))
        
        print(f"    ğŸ” Attention patch range: {patch_start_attention}-{patch_end_attention} (out of {num_time_patches})")
        
        # í•´ë‹¹ êµ¬ê°„ì—ì„œì˜ ì£¼íŒŒìˆ˜ë³„ ì–´í…ì…˜ í‰ê·  (ë” ì •í™•í•œ ë§¤í•‘)
        patch_attention_freq = np.mean(attention_weights[:, patch_start_attention:patch_end_attention], axis=1)
        
        # ì–´í…ì…˜ íŒ¨ì¹˜ë¥¼ STFT ì£¼íŒŒìˆ˜ ë¹ˆìœ¼ë¡œ ë§¤í•‘ (ê°œì„ ëœ ë§¤í•‘)
        num_attention_freq_patches = attention_weights.shape[0]
        stft_freq_bins = len(base_template)
        
        print(f"    ğŸ” Mapping {num_attention_freq_patches} attention patches to {stft_freq_bins} STFT bins")
        
        for i in range(stft_freq_bins):
            # ë” ì •í™•í•œ ë§¤í•‘: STFT ì£¼íŒŒìˆ˜ ë¹ˆì„ ì–´í…ì…˜ íŒ¨ì¹˜ë¡œ ë³€í™˜
            attention_patch_idx = int(i * num_attention_freq_patches / stft_freq_bins)
            attention_patch_idx = min(attention_patch_idx, num_attention_freq_patches - 1)
            attention_weights[i] = patch_attention_freq[attention_patch_idx]
        
        # ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì •ê·œí™” ë° ê°•í™”
        attention_weights = np.maximum(attention_weights, 0.1)  # ìµœì†Œê°’ ë³´ì¥
        attention_weights = attention_weights / (np.max(attention_weights) + 1e-8)  # ì •ê·œí™”
        
        print(f"    ğŸ” Attention weights range: {attention_weights.min():.3f} - {attention_weights.max():.3f}")
        print(f"    ğŸ” High attention frequencies: {np.sum(attention_weights > 0.7)}/{len(attention_weights)}")
    
    # 3. ì§„í­ ê°•ì¡°ëœ ì£¼íŒŒìˆ˜ ì‹ë³„
    amplitude_threshold = np.percentile(base_template, 75)  # ìƒìœ„ 25%
    amplitude_mask = base_template >= amplitude_threshold
    
    # 4. ì–´í…ì…˜ê³¼ ì§„í­ì„ ê²°í•©í•œ ìµœì¢… í…œí”Œë¦¿ (ê°œì„ )
    template = base_template.copy()
    
    # ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ê°•í™” ì ìš© (ë” ê°•í•œ ë°˜ì˜)
    attention_factor = 0.3 + 0.7 * attention_weights  # 0.3~1.0 ë²”ìœ„
    template *= attention_factor
    
    # ì§„í­ ê°•ì¡°ëœ ì£¼íŒŒìˆ˜ ì¶”ê°€ ê°•í™”
    template[amplitude_mask] *= 1.8  # 1.5 â†’ 1.8ë¡œ ì¦ê°€
    
    # ì–´í…ì…˜ê³¼ ì§„í­ì´ ëª¨ë‘ ë†’ì€ ì£¼íŒŒìˆ˜ íŠ¹ë³„ ê°•í™”
    high_attention_mask = attention_weights > 0.7
    combined_mask = amplitude_mask & high_attention_mask
    template[combined_mask] *= 2.0  # ì–´í…ì…˜+ì§„í­ ëª¨ë‘ ë†’ì€ ì£¼íŒŒìˆ˜ 2ë°° ê°•í™”
    
    print(f"    ğŸ” Template enhancement: attention factor range {attention_factor.min():.3f}-{attention_factor.max():.3f}")
    print(f"    ğŸ” Amplitude-emphasized frequencies: {np.sum(amplitude_mask)}")
    print(f"    ğŸ” High attention frequencies: {np.sum(high_attention_mask)}")
    print(f"    ğŸ” Combined high attention+amplitude: {np.sum(combined_mask)}")
    
    # í…œí”Œë¦¿ ì •ê·œí™” (ì–‘ìˆ˜ ë³´ì¥)
    template = np.maximum(template, 1e-8)
    template_norm = template / (np.sum(template) + 1e-8)
    
    print(f"    ğŸ” Template created: {len(template)} frequency bins, energy: {np.sum(template**2):.3f}")
    print(f"    ğŸ” Template time span: {actual_patch_size} frames ({actual_patch_size * HOP / SR:.3f}s)")
    print(f"    ğŸ” Attention-weighted template with {np.sum(amplitude_mask)}/{len(template)} amplitude-emphasized frequencies")
    
    # === ìˆ˜ì • 3: ìœ ë™ì  í¬ê¸° í…œí”Œë¦¿ ê¸°ë°˜ ìœ ì‚¬ë„ ê³„ì‚° ===
    num_frames = stft_mag.shape[1]
    similarity = np.zeros(num_frames)
    
    print(f"    ğŸ” Computing similarity for {num_frames} frames using {actual_patch_size}-frame template")
    print(f"    ğŸ” Template represents minimal time pattern for continuous comparison")
    
    for t in range(num_frames):
        current = stft_mag[:, t]
        
        # í˜„ì¬ í”„ë ˆì„ ì •ê·œí™” (ì–‘ìˆ˜ ë³´ì¥)
        current = np.maximum(current, 1e-8)
        current_norm = current / (np.sum(current) + 1e-8)
        
        # 1. ì½”ì‚¬ì¸ ìœ ì‚¬ë„ (ê°€ì¥ ì•ˆì •ì )
        cosine_sim = np.dot(template_norm, current_norm) / (
            np.linalg.norm(template_norm) * np.linalg.norm(current_norm) + 1e-8
        )
        cosine_sim = np.clip(cosine_sim, 0, 1)
        
        # 2. ì—ë„ˆì§€ ë¹„ìœ¨ ìœ ì‚¬ë„
        template_energy = np.sum(template ** 2)
        current_energy = np.sum(current ** 2)
        if template_energy > 1e-8 and current_energy > 1e-8:
            energy_ratio = min(template_energy, current_energy) / max(template_energy, current_energy)
        else:
            energy_ratio = 0
        
        # 3. ìŠ¤í™íŠ¸ëŸ´ ì¤‘ì‹¬ ìœ ì‚¬ë„
        freqs = np.arange(len(template))
        template_centroid = np.sum(freqs * template_norm)
        current_centroid = np.sum(freqs * current_norm)
        centroid_diff = abs(template_centroid - current_centroid) / len(template)
        centroid_sim = np.exp(-centroid_diff * 3)  # ë” ê°•í•œ ê°€ì¤‘ì¹˜
        
        # 4. ì£¼íŒŒìˆ˜ ë¶„í¬ ìœ ì‚¬ë„ (ì¶”ê°€)
        # í…œí”Œë¦¿ê³¼ í˜„ì¬ í”„ë ˆì„ì˜ ì£¼íŒŒìˆ˜ ë¶„í¬ ë¹„êµ
        freq_dist_sim = np.corrcoef(template_norm, current_norm)[0, 1]
        if np.isnan(freq_dist_sim):
            freq_dist_sim = 0
        freq_dist_sim = np.clip(freq_dist_sim, 0, 1)
        
        # ì¢…í•© ìœ ì‚¬ë„ (ê°€ì¤‘ì¹˜ ì¡°ì • - ì£¼íŒŒìˆ˜ ë¶„í¬ ì¶”ê°€)
        similarity[t] = 0.4 * cosine_sim + 0.2 * energy_ratio + 0.2 * centroid_sim + 0.2 * freq_dist_sim
        similarity[t] = np.clip(similarity[t], 0, 1)
    
    # === ìˆ˜ì • 3: ì•µì»¤ êµ¬ê°„ë„ ì‹¤ì œ ìœ ì‚¬ë„ ê³„ì‚° (ê°•ì œ ë³´ì • ì œê±°) ===
    # ì•µì»¤ êµ¬ê°„ë„ ì‹¤ì œ ìœ ì‚¬ë„ë¡œ ê³„ì‚°í•˜ì—¬ ìì—°ìŠ¤ëŸ¬ìš´ ë¹„êµ ìˆ˜í–‰
    # similarity[anchor_start:anchor_end] = 1.0  # ì´ ë¶€ë¶„ ì œê±°
    
    # === ìˆ˜ì • 4: ì‹œê°„ì  ìŠ¤ë¬´ë”© (ê¸‰ê²©í•œ ë³€í™” ë°©ì§€) ===
    similarity = apply_smoothing_filters(similarity, is_2d=False)
    
    # ì•µì»¤ êµ¬ê°„ ì¬ë³´ì • ì œê±° - ì‹¤ì œ ìœ ì‚¬ë„ ìœ ì§€
    # similarity[anchor_start:anchor_end] = np.maximum(
    #     similarity[anchor_start:anchor_end], 0.95
    # )
    
    # === ìˆ˜ì • 5: 2D ë§ˆìŠ¤í¬ ìƒì„± ===
    mask = np.zeros_like(stft_mag)
    
    for t in range(num_frames):
        if similarity[t] >= SeparationConfig.SIMILARITY_THRESHOLD:
            # ë†’ì€ ìœ ì‚¬ë„: ë§¤ìš° ê°•í•˜ê²Œ ë¶„ë¦¬
            mask[:, t] = similarity[t] * 1.8  # 80% ì¶”ê°€ ê°•í™”
        elif similarity[t] >= SeparationConfig.SIMILARITY_THRESHOLD * 0.7:
            # ì¤‘ê°„-ë†’ì€ ìœ ì‚¬ë„: ê°•í•˜ê²Œ ë¶„ë¦¬
            mask[:, t] = similarity[t] * 1.2  # 20% ì¶”ê°€ ê°•í™”
        elif similarity[t] >= SeparationConfig.SIMILARITY_THRESHOLD * 0.4:
            # ì¤‘ê°„ ìœ ì‚¬ë„: ë³´í†µ ê°•ë„
            mask[:, t] = similarity[t] * 0.6
        elif similarity[t] >= SeparationConfig.SIMILARITY_THRESHOLD * 0.2:
            # ë‚®ì€ ìœ ì‚¬ë„: ì•„ì£¼ ì•½í•˜ê²Œ ë¶„ë¦¬
            mask[:, t] = similarity[t] * 0.15
    else:
            # ë§¤ìš° ë‚®ì€ ìœ ì‚¬ë„: ê±°ì˜ ë¶„ë¦¬í•˜ì§€ ì•ŠìŒ
            mask[:, t] = similarity[t] * 0.05
    
    # === ìˆ˜ì • 6: ì£¼íŒŒìˆ˜ë³„ ê°€ì¤‘ì¹˜ ì ìš© ë° ì›ë³¸ ì§„í­ ì œí•œ ===
    if anchor_spec.size > 0:
        # ì•µì»¤ì—ì„œ ì¤‘ìš”í•œ ì£¼íŒŒìˆ˜ ì‹ë³„ (ë” ê°•í•œ ê°€ì¤‘ì¹˜)
        freq_importance = np.percentile(anchor_spec, 80, axis=1)  # 80 percentileë¡œ ìƒí–¥
        freq_importance = freq_importance / (np.max(freq_importance) + 1e-8)
        
        # ì¤‘ìš” ì£¼íŒŒìˆ˜ ê°•ì¡° (ì–´í…ì…˜ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ì ìš©)
        for f in range(mask.shape[0]):
            # ê¸°ë³¸ ì¤‘ìš”ë„ ê°€ì¤‘ì¹˜
            importance_factor = 0.5 + 1.0 * freq_importance[f]
            
            # ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì¶”ê°€ (í•´ë‹¹ ì£¼íŒŒìˆ˜ê°€ ì–´í…ì…˜ì—ì„œ ë†’ì€ ê²½ìš°)
            if f < len(attention_weights):
                attention_factor = 0.8 + 0.4 * attention_weights[f]  # 0.8~1.2 ë²”ìœ„
            else:
                attention_factor = 1.0
            
            # ìµœì¢… ê°€ì¤‘ì¹˜ = ì¤‘ìš”ë„ Ã— ì–´í…ì…˜
            weight_factor = importance_factor * attention_factor
            mask[f, :] *= weight_factor
        
        # ì›ë³¸ ì˜¤ë””ì˜¤ ì§„í­ ì œí•œ: ë§ˆìŠ¤í¬ê°€ ì›ë³¸ì„ ë„˜ì§€ ì•Šë„ë¡
        # ê° ì‹œê°„ í”„ë ˆì„ë³„ë¡œ ì›ë³¸ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ ëŒ€ë¹„ ë§ˆìŠ¤í¬ ê°•ë„ ì œí•œ
        for t in range(mask.shape[1]):
            original_energy = np.sum(stft_mag[:, t] ** 2)
            if original_energy > 1e-8:
                # ë§ˆìŠ¤í¬ ì ìš© í›„ ì—ë„ˆì§€ê°€ ì›ë³¸ì„ ë„˜ì§€ ì•Šë„ë¡ ì œí•œ (100%)
                max_energy_ratio = 1.0
                current_masked_energy = np.sum((stft_mag[:, t] * mask[:, t]) ** 2)
                
                if current_masked_energy > original_energy * max_energy_ratio:
                    # ìŠ¤ì¼€ì¼ë§ íŒ©í„° ê³„ì‚°
                    scale_factor = np.sqrt(original_energy * max_energy_ratio / current_masked_energy)
                    mask[:, t] *= scale_factor
                    
                    print(f"    ğŸ”§ Frame {t}: Energy limited to {max_energy_ratio*100}% of original")
        
        print(f"    ğŸ“Š Frequency weighting applied: {np.sum(freq_importance > 0.5)}/{len(freq_importance)} important bins")
    
    # ìµœì¢… ì •ê·œí™” ë° ìŠ¤ë¬´ë”© (ì£¼íŒŒìˆ˜ì™€ ì‹œê°„ ëª¨ë‘)
    mask = apply_smoothing_filters(mask, is_2d=True)
    
    print(f"    ğŸ” Enhanced smoothing applied: frequency axis (Ïƒ=1.5), time axis (Ïƒ=1.0), 2D (Ïƒ=0.8,0.6)")
    
    return mask, similarity

# ==================== Wiener í•„í„°ë§ ====================
def apply_adaptive_wiener(
    stft: np.ndarray,
    mask: np.ndarray,
    anchor_frames: Tuple[int, int]
) -> np.ndarray:
    """ì ì‘ì  Wiener í•„í„°ë§ - tanh/sigmoid ê¸°ë°˜ ê°•ë„ ì¡°ì ˆ"""
    anchor_start, anchor_end = anchor_frames
    
    # ë…¸ì´ì¦ˆ ì¶”ì •
    noise_mask = 1.0 - mask
    noise_power = np.abs(stft) ** 2 * noise_mask
    noise_estimate = np.mean(noise_power, axis=1, keepdims=True) + 1e-8
    
    # ì‹ í˜¸ íŒŒì›Œ
    signal_power = np.abs(stft) ** 2 * mask
    
    # ê¸°ë³¸ Wiener gain ê³„ì‚°
    wiener_gain = signal_power / (signal_power + noise_estimate)
    wiener_gain = np.clip(wiener_gain, 0, 1)
    
    # tanh ê¸°ë°˜ ê°•ë„ ì¡°ì ˆ: ë†’ì€ ë¶€ë¶„ì€ ê°•í•˜ê²Œ, ì•½í•œ ë¶€ë¶„ì€ ì•½í•˜ê²Œ
    # mask ê°’ì— ë”°ë¼ tanh í•¨ìˆ˜ë¡œ ê°•ë„ ì¡°ì ˆ
    # maskê°€ ë†’ì„ìˆ˜ë¡ (1ì— ê°€ê¹Œìš¸ìˆ˜ë¡) ë” ê°•í•œ í•„í„°ë§ ì ìš©
    
    # 1. tanh ê¸°ë°˜ ê°•ë„ ì¡°ì ˆ (ì„¤ì • ê°€ëŠ¥í•œ ìŠ¤ì¼€ì¼ë§)
    intensity_factor = 0.5 + 0.5 * np.tanh(WienerConfig.TANH_SCALE * (mask - 0.5))
    
    # 2. sigmoid ê¸°ë°˜ ì¶”ê°€ ê°•í™” (ì„¤ì • ê°€ëŠ¥í•œ ì„ê³„ê°’ê³¼ ìŠ¤ì¼€ì¼)
    sigmoid_factor = 1 / (1 + np.exp(-WienerConfig.SIGMOID_SCALE * (mask - WienerConfig.SIGMOID_THRESHOLD)))
    
    # 3. ìµœì¢… ê°•ë„ ì¡°ì ˆ (tanh + sigmoid ê²°í•©)
    final_intensity = intensity_factor * (0.7 + 0.3 * sigmoid_factor)
    
    # 4. Wiener gainì— ê°•ë„ ì ìš©
    enhanced_wiener_gain = wiener_gain * final_intensity
    
    # 5. ì•µì»¤ êµ¬ê°„ íŠ¹ë³„ ì²˜ë¦¬ (ì„¤ì • ê°€ëŠ¥í•œ ë¶€ìŠ¤íŠ¸)
    enhanced_wiener_gain[:, anchor_start:anchor_end] *= WienerConfig.ANCHOR_BOOST
    
    # 6. ìµœì¢… í´ë¦¬í•‘
    enhanced_wiener_gain = np.clip(enhanced_wiener_gain, 0, 1)
    
    print(f"    ğŸ” Wiener filtering: tanh/sigmoid intensity applied")
    print(f"    ğŸ” Intensity range: {final_intensity.min():.3f} - {final_intensity.max():.3f}")
    print(f"    ğŸ” Anchor boost: {WienerConfig.ANCHOR_BOOST}x")
    
    return stft * enhanced_wiener_gain

# ==================== ë‹¨ì¼ íŒ¨ìŠ¤ ì²˜ë¦¬ ====================
def process_single_pass(
    audio_4sec: np.ndarray,
    audio_10sec: np.ndarray,
    ast_processor: ASTProcessor,
    suppressed_regions: List[Tuple[int, int]],
    previous_peaks: List[Tuple[int, int]],
    pass_idx: int
) -> Optional[SeparationResult]:
    """ë‹¨ì¼ íŒ¨ìŠ¤ ìŒì› ë¶„ë¦¬"""
    
    print(f"  ì²˜ë¦¬ ì¤‘...")
    
    # AST ì²˜ë¦¬ (10.24ì´ˆ)
    attention_matrix, classification, ast_spec = ast_processor.process(audio_10sec)
    
    # STFT (4.096ì´ˆ)
    stft, mag, phase = compute_stft(audio_4sec)
    
    # ì•µì»¤ ì„ ì •
    anchor_start, anchor_end, is_valid, anchor_freq_idx = find_attention_anchor(
        attention_matrix, suppressed_regions, previous_peaks, len(audio_4sec)
    )
    
    if not is_valid:
        return None
    
    anchor_time = (anchor_start * HOP / SR, anchor_end * HOP / SR)
    print(f"  ì•µì»¤: {anchor_time[0]:.3f}s - {anchor_time[1]:.3f}s")
    
    # í…œí”Œë¦¿ ë§ˆìŠ¤í‚¹
    mask, similarity = create_template_mask(mag, (anchor_start, anchor_end), attention_matrix)
    
    # Wiener í•„í„°ë§
    filtered_stft = apply_adaptive_wiener(stft, mask, (anchor_start, anchor_end))
    
    # ì—­ë³€í™˜
    separated = librosa.istft(filtered_stft, hop_length=HOP, win_length=WINLEN)
    
    # ê¸¸ì´ ì¡°ì •
    if len(separated) < L_INPUT:
        separated = np.pad(separated, (0, L_INPUT - len(separated)))
    else:
        separated = separated[:L_INPUT]
    
    # ì”ì—¬ ê³„ì‚°
    residual = audio_4sec - separated
    
    # ì—ë„ˆì§€ ë¹„ìœ¨
    energy_ratio = np.sum(separated ** 2) / (np.sum(audio_4sec ** 2) + 1e-8)
    
    return SeparationResult(
        separated_audio=separated,
        residual_audio=residual,
        mask=mask,
        anchor_frames=(anchor_start, anchor_end),
        energy_ratio=energy_ratio,
        classification=classification,
        attention_matrix=attention_matrix,
        template_similarity=similarity
    )

# ==================== ë©€í‹°íŒ¨ìŠ¤ ë¶„ë¦¬ ====================
def multi_pass_separation(
    audio_4sec: np.ndarray,
    audio_10sec: np.ndarray,
    ast_processor: ASTProcessor,
    max_passes: int = 2
) -> List[SeparationResult]:
    """ë©€í‹°íŒ¨ìŠ¤ ìŒì› ë¶„ë¦¬"""
    results = []
    suppressed_regions = []
    previous_peaks = []
    
    current_4sec = audio_4sec.copy()
    current_10sec = audio_10sec.copy()
    
    for i in range(max_passes):
        print(f"\n=== Pass {i + 1}/{max_passes} ===")
        
        # ë””ë²„ê·¸: í˜„ì¬ ì‚¬ìš©í•  ì˜¤ë””ì˜¤ ì •ë³´ ì¶œë ¥
        if i == 0:
            print(f"  ğŸ” Using original audio: {len(current_4sec)} samples")
        else:
            print(f"  ğŸ” Using residual audio: {len(current_4sec)} samples")
            print(f"  ğŸ” Residual energy: {np.sum(current_4sec**2):.2f}")
        
        result = process_single_pass(
            current_4sec, current_10sec, ast_processor,
            suppressed_regions, previous_peaks, i
        )
        
        if result is None or result.energy_ratio < 0.0001:  # ì„ê³„ê°’ì„ ë”ìš± ë‚®ì¶¤ (0.0005 â†’ 0.0001)
            print(f"  ì¢…ë£Œ: ì—ë„ˆì§€ ë¶€ì¡± (ratio: {result.energy_ratio if result else 0:.6f})")
            break
        
        # ìƒìœ„ ì–´í…ì…˜ ì˜ì—­ ì €ì¥
        att_mean = np.mean(result.attention_matrix, axis=0)
        threshold = np.percentile(att_mean, SeparationConfig.ATTENTION_PERCENTILE)
        peaks = np.where(att_mean > threshold)[0]
        if len(peaks) > 0:
            previous_peaks.append((peaks[0], peaks[-1]))
        
        results.append(result)
        suppressed_regions.append(result.anchor_frames)
        
        # ì”ì—¬ ì˜¤ë””ì˜¤ë¡œ ì—…ë°ì´íŠ¸
        current_4sec = result.residual_audio
        current_10sec = np.pad(result.residual_audio, (0, max(0, L_MODEL - len(result.residual_audio))))[:L_MODEL]
        
        print(f"  í´ë˜ìŠ¤: {result.classification['predicted_class']}")
        print(f"  ì‹ ë¢°ë„: {result.classification['confidence']:.3f}")
        print(f"  ì—ë„ˆì§€: {result.energy_ratio:.3f}")
    
    return results

# ==================== ì‹œê°í™” ====================
def create_debug_plot(
    original_audio: np.ndarray,
    results: List[SeparationResult],
    output_dir: Path
):
    """ë””ë²„ê·¸ ì‹œê°í™”"""
    for idx, result in enumerate(results):
        # ê° íŒ¨ìŠ¤ë³„ë¡œ ì˜¬ë°”ë¥¸ ì…ë ¥ ì˜¤ë””ì˜¤ ì„ íƒ
        if idx == 0:
            # Pass 1: ì›ë³¸ ì˜¤ë””ì˜¤
            input_audio = original_audio
        else:
            # Pass 2+: ì´ì „ íŒ¨ìŠ¤ì˜ ì”ì—¬ ì˜¤ë””ì˜¤
            input_audio = results[idx-1].residual_audio
        fig = plt.figure(figsize=(16, 10))
        
        # ë ˆì´ì•„ì›ƒ ì„¤ì •
        gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)
        
        # ì‹œê°„ì¶•
        time_axis = np.arange(len(input_audio)) / SR
        
        # 1. ì…ë ¥ ì˜¤ë””ì˜¤ (Pass 1: ì›ë³¸, Pass 2+: ì”ì—¬)
        ax1 = fig.add_subplot(gs[0, 0])
        ax1.plot(time_axis, input_audio, 'b-', alpha=0.7)
        if idx == 0:
            ax1.set_title('Original Audio')
        else:
            ax1.set_title(f'Residual Audio from Pass {idx}')
        ax1.set_xlabel('Time (s)')
        ax1.set_ylabel('Amplitude')
        ax1.set_ylim(-1, 1)  # -1ì—ì„œ 1 ë²”ìœ„ë¡œ í†µì¼
        ax1.grid(True, alpha=0.3)
        
        # 2. ë¶„ë¦¬ëœ ì˜¤ë””ì˜¤
        ax2 = fig.add_subplot(gs[0, 1])
        ax2.plot(time_axis, result.separated_audio, 'g-', alpha=0.7)
        ax2.set_title(f'Separated: {result.classification["predicted_class"]}')
        ax2.set_xlabel('Time (s)')
        ax2.set_ylabel('Amplitude')
        ax2.set_ylim(-1, 1)  # -1ì—ì„œ 1 ë²”ìœ„ë¡œ í†µì¼
        ax2.grid(True, alpha=0.3)
        
        # 3. ì”ì—¬ ì˜¤ë””ì˜¤
        ax3 = fig.add_subplot(gs[0, 2])
        ax3.plot(time_axis, result.residual_audio, 'r-', alpha=0.7)
        ax3.set_title('Residual Audio')
        ax3.set_xlabel('Time (s)')
        ax3.set_ylabel('Amplitude')
        ax3.set_ylim(-1, 1)  # -1ì—ì„œ 1 ë²”ìœ„ë¡œ í†µì¼
        ax3.grid(True, alpha=0.3)
        
        # 4. í…œí”Œë¦¿ ìœ ì‚¬ë„
        ax4 = fig.add_subplot(gs[1, 0])
        time_frames = np.arange(len(result.template_similarity)) * HOP / SR
        ax4.plot(time_frames, result.template_similarity, 'purple', linewidth=2)
        # 5ë‹¨ê³„ ìœ ì‚¬ë„ ì„ê³„ê°’ í‘œì‹œ
        ax4.axhline(y=SeparationConfig.SIMILARITY_THRESHOLD, color='red', linestyle='-', linewidth=2,
                   label=f'High={SeparationConfig.SIMILARITY_THRESHOLD}')
        ax4.axhline(y=SeparationConfig.SIMILARITY_THRESHOLD * 0.7, color='orange', linestyle='--', 
                   label=f'Med-High={SeparationConfig.SIMILARITY_THRESHOLD * 0.7:.2f}')
        ax4.axhline(y=SeparationConfig.SIMILARITY_THRESHOLD * 0.4, color='yellow', linestyle=':', 
                   label=f'Medium={SeparationConfig.SIMILARITY_THRESHOLD * 0.4:.2f}')
        ax4.axhline(y=SeparationConfig.SIMILARITY_THRESHOLD * 0.2, color='lightblue', linestyle=':', 
                   label=f'Weak={SeparationConfig.SIMILARITY_THRESHOLD * 0.2:.2f}')
        
        # 5ë‹¨ê³„ ìƒ‰ìƒ êµ¬ë¶„
        ax4.fill_between(time_frames, 0, result.template_similarity, 
                         where=(result.template_similarity >= SeparationConfig.SIMILARITY_THRESHOLD),
                         color='red', alpha=0.4, label='Very High (1.8x)')
        ax4.fill_between(time_frames, 0, result.template_similarity, 
                         where=((result.template_similarity >= SeparationConfig.SIMILARITY_THRESHOLD * 0.7) & 
                                (result.template_similarity < SeparationConfig.SIMILARITY_THRESHOLD)),
                         color='orange', alpha=0.3, label='High (1.2x)')
        ax4.fill_between(time_frames, 0, result.template_similarity, 
                         where=((result.template_similarity >= SeparationConfig.SIMILARITY_THRESHOLD * 0.4) & 
                                (result.template_similarity < SeparationConfig.SIMILARITY_THRESHOLD * 0.7)),
                         color='yellow', alpha=0.2, label='Medium (0.6x)')
        ax4.fill_between(time_frames, 0, result.template_similarity, 
                         where=((result.template_similarity >= SeparationConfig.SIMILARITY_THRESHOLD * 0.2) & 
                                (result.template_similarity < SeparationConfig.SIMILARITY_THRESHOLD * 0.4)),
                         color='lightblue', alpha=0.1, label='Weak (0.15x)')
        ax4.fill_between(time_frames, 0, result.template_similarity, 
                         where=(result.template_similarity < SeparationConfig.SIMILARITY_THRESHOLD * 0.2),
                         color='lightgray', alpha=0.05, label='Very Weak (0.05x)')
        ax4.set_title('Template Similarity')
        ax4.set_xlabel('Time (s)')
        ax4.set_ylabel('Similarity')
        ax4.legend()
        ax4.grid(True, alpha=0.3)
        
        # 5. ë¶„ë¦¬ ë§ˆìŠ¤í¬
        ax5 = fig.add_subplot(gs[1, 1])
        im5 = ax5.imshow(result.mask, aspect='auto', origin='lower', cmap='hot', interpolation='bilinear')
        ax5.set_title('Separation Mask')
        ax5.set_xlabel('Time frames')
        ax5.set_ylabel('Frequency bins')
        plt.colorbar(im5, ax=ax5, fraction=0.046)
        
        # ì•µì»¤ ì˜ì—­ í‘œì‹œ
        anchor_start, anchor_end = result.anchor_frames
        ax5.axvline(x=anchor_start, color='cyan', linestyle='--', linewidth=2)
        ax5.axvline(x=anchor_end, color='cyan', linestyle='--', linewidth=2)
        
        # 6. STFT ìŠ¤í™íŠ¸ë¡œê·¸ë¨
        ax6 = fig.add_subplot(gs[1, 2])
        _, mag, _ = compute_stft(input_audio)
        db_spec = librosa.amplitude_to_db(mag, ref=np.max)
        im6 = ax6.imshow(db_spec, aspect='auto', origin='lower', cmap='magma', interpolation='bilinear')
        ax6.set_title('STFT Spectrogram (dB)')
        ax6.set_xlabel('Time frames')
        ax6.set_ylabel('Frequency bins')
        plt.colorbar(im6, ax=ax6, fraction=0.046)
        
        # 7. ì–´í…ì…˜ ë§¤íŠ¸ë¦­ìŠ¤ (ì°¨ì› ì¼ì¹˜)
        ax7 = fig.add_subplot(gs[2, 0])
        # ì–´í…ì…˜ ë§¤íŠ¸ë¦­ìŠ¤ëŠ” [freq_patches, time_patches] í˜•íƒœ
        # imshowëŠ” ê¸°ë³¸ì ìœ¼ë¡œ origin='upper'ì´ë¯€ë¡œ ì£¼íŒŒìˆ˜ê°€ ìœ„ì—ì„œ ì•„ë˜ë¡œ
        # ë‹¤ë¥¸ ì‹œê°í™”ì™€ ì¼ì¹˜ì‹œí‚¤ê¸° ìœ„í•´ origin='lower'ë¡œ ì„¤ì •
        im7 = ax7.imshow(result.attention_matrix, aspect='auto', origin='lower', cmap='coolwarm', interpolation='bilinear')
        ax7.set_title('AST Attention Matrix (Frequency: 0â†’High)')
        ax7.set_xlabel('Time patches')
        ax7.set_ylabel('Frequency patches (0â†’High)')
        
        # 4.096ì´ˆ ê²½ê³„ í‘œì‹œ
        valid_patches = int(INPUT_SEC / MODEL_SEC * AST_TIME_PATCHES)
        ax7.axvline(x=valid_patches, color='green', linestyle='--', linewidth=2, label='4.096s')
        
        # ì•µì»¤ ì˜ì—­ í‘œì‹œ (4.096ì´ˆ ì‹œê°„ëŒ€ì— ë§ì¶°)
        anchor_start, anchor_end = result.anchor_frames
        # STFT í”„ë ˆì„ì„ ì–´í…ì…˜ íŒ¨ì¹˜ë¡œ ë³€í™˜ (4.096ì´ˆ ê¸°ì¤€)
        # 4.096ì´ˆì— í•´ë‹¹í•˜ëŠ” ìœ íš¨ íŒ¨ì¹˜ ìˆ˜ ê³„ì‚°
        valid_patches = int(INPUT_SEC / MODEL_SEC * AST_TIME_PATCHES)
        anchor_start_patch = int(anchor_start * valid_patches / (L_INPUT // HOP))
        anchor_end_patch = int(anchor_end * valid_patches / (L_INPUT // HOP))
        anchor_start_patch = max(0, min(valid_patches-1, anchor_start_patch))
        anchor_end_patch = max(0, min(valid_patches, anchor_end_patch))
        
        # ì•µì»¤ ì˜ì—­ ë¹¨ê°„ìƒ‰ ë°•ìŠ¤ë¡œ í‘œì‹œ
        from matplotlib.patches import Rectangle
        rect = Rectangle((anchor_start_patch, 0), anchor_end_patch - anchor_start_patch, 
                        result.attention_matrix.shape[0], linewidth=3, 
                        edgecolor='red', facecolor='none', alpha=0.8)
        ax7.add_patch(rect)
        
        # ì•µì»¤ í…ìŠ¤íŠ¸ í‘œì‹œ
        ax7.text(anchor_start_patch + (anchor_end_patch - anchor_start_patch) / 2, 
                result.attention_matrix.shape[0] * 0.9,
                f'', 
                ha='center', va='top', fontsize=10, color='red', 
                bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))
        
        ax7.legend()
        plt.colorbar(im7, ax=ax7, fraction=0.046)
        
        # 8. ì—ë„ˆì§€ ë¶„í¬
        ax8 = fig.add_subplot(gs[2, 1])
        energies = {
            'Input': np.sum(input_audio ** 2),
            'Separated': np.sum(result.separated_audio ** 2),
            'Residual': np.sum(result.residual_audio ** 2)
        }
        bars = ax8.bar(energies.keys(), energies.values(), color=['blue', 'green', 'red'])
        ax8.set_title('Energy Distribution')
        ax8.set_ylabel('Energy')
        
        # ì—ë„ˆì§€ ë¹„ìœ¨ í‘œì‹œ
        for bar, (name, value) in zip(bars, energies.items()):
            height = bar.get_height()
            ax8.text(bar.get_x() + bar.get_width()/2., height,
                    f'{value/energies["Input"]*100:.1f}%',
                    ha='center', va='bottom')
        
        # 9. ë¶„ë¥˜ ê²°ê³¼
        ax9 = fig.add_subplot(gs[2, 2])
        ax9.axis('off')
        
        # ë¶„ë¥˜ ì •ë³´ í…ìŠ¤íŠ¸
        info_text = f"Classification Results\n" + "="*30 + "\n"
        info_text += f"Predicted: {result.classification['predicted_class']}\n"
        info_text += f"Confidence: {result.classification['confidence']:.2%}\n\n"
        info_text += "Top 5 Classes:\n"
        for cls, prob in zip(result.classification['top5_classes'][:5], 
                            result.classification['top5_probs'][:5]):
            info_text += f"  â€¢ {cls}: {prob:.2%}\n"
        info_text += f"\nEnergy Ratio: {result.energy_ratio:.3f}\n"
        info_text += f"Anchor: {anchor_start*HOP/SR:.3f}s - {anchor_end*HOP/SR:.3f}s"
        
        ax9.text(0.1, 0.9, info_text, transform=ax9.transAxes,
                fontsize=10, verticalalignment='top',
                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
        
        # ì „ì²´ ì œëª©
        fig.suptitle(f'Pass {idx + 1} - Audio Source Separation Analysis', fontsize=14, fontweight='bold')
        
        # ì €ì¥
        output_path = output_dir / f'debug_pass_{idx+1}.png'
        plt.savefig(output_path, dpi=100, bbox_inches='tight')
        plt.close()
        
        print(f"  ì‹œê°í™” ì €ì¥: {output_path}")

# ==================== ë©”ì¸ íŒŒì´í”„ë¼ì¸ ====================
class SourceSeparationPipeline:
    """AST ê¸°ë°˜ ìŒì› ë¶„ë¦¬ íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self, output_dir: str = "output"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True, parents=True)
        self.ast_processor = ASTProcessor()
        
    def process(self, audio_path: str, visualize: bool = True) -> Dict[str, Any]:
        """ì˜¤ë””ì˜¤ íŒŒì¼ ì²˜ë¦¬"""
        print(f"\n{'='*60}")
        print(f"ì²˜ë¦¬ ì‹œì‘: {audio_path}")
        print(f"{'='*60}")
        
        # ì˜¤ë””ì˜¤ ë¡œë“œ
        audio_4sec, audio_10sec = load_audio(audio_path)
        print(f"ì˜¤ë””ì˜¤ ë¡œë“œ: {INPUT_SEC}ì´ˆ â†’ {MODEL_SEC}ì´ˆ íŒ¨ë”©")
        
        # ë©€í‹°íŒ¨ìŠ¤ ë¶„ë¦¬
        results = multi_pass_separation(audio_4sec, audio_10sec, self.ast_processor)
        
        # ê²°ê³¼ ì €ì¥
        output_data = {
            'input_file': str(audio_path),
            'input_duration': INPUT_SEC,
            'model_duration': MODEL_SEC,
            'num_sources': len(results),
            'sources': []
        }
        
        print(f"\n{'='*60}")
        print(f"ê²°ê³¼ ì €ì¥")
        print(f"{'='*60}")
        
        for idx, result in enumerate(results, 1):
            # ì˜¤ë””ì˜¤ ì €ì¥
            audio_path = self.output_dir / f'separated_{idx}.wav'
            sf.write(audio_path, result.separated_audio, SR)
            
            # ë©”íƒ€ë°ì´í„°
            anchor_time_start = result.anchor_frames[0] * HOP / SR
            anchor_time_end = result.anchor_frames[1] * HOP / SR
            
            source_info = {
                'index': idx,
                'audio_file': str(audio_path),
                'class': result.classification['predicted_class'],
                'confidence': float(result.classification['confidence']),
                'top5_classes': result.classification['top5_classes'],
                'top5_probs': result.classification['top5_probs'],
                'energy_ratio': float(result.energy_ratio),
                'anchor_time': {
                    'start': float(anchor_time_start),
                    'end': float(anchor_time_end),
                    'duration': float(anchor_time_end - anchor_time_start)
                }
            }
            output_data['sources'].append(source_info)
            
            print(f"ì†ŒìŠ¤ {idx}:")
            print(f"  - í´ë˜ìŠ¤: {source_info['class']}")
            print(f"  - ì‹ ë¢°ë„: {source_info['confidence']:.2%}")
            print(f"  - ì—ë„ˆì§€: {source_info['energy_ratio']:.2%}")
            print(f"  - íŒŒì¼: {audio_path}")
        
        # ì”ì—¬ ì˜¤ë””ì˜¤ ì €ì¥
        if results:
            residual_path = self.output_dir / 'residual.wav'
            sf.write(residual_path, results[-1].residual_audio, SR)
            output_data['residual_file'] = str(residual_path)
            print(f"ì”ì—¬: {residual_path}")
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥
        metadata_path = self.output_dir / 'metadata.json'
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, indent=2, ensure_ascii=False)
        print(f"ë©”íƒ€ë°ì´í„°: {metadata_path}")
        
        # ì‹œê°í™”
        if visualize:
            print(f"\nì‹œê°í™” ìƒì„± ì¤‘...")
            create_debug_plot(audio_4sec, results, self.output_dir)
        
        print(f"\n{'='*60}")
        print(f"ì²˜ë¦¬ ì™„ë£Œ!")
        print(f"{'='*60}")
        
        return output_data

# ==================== ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ====================
def analyze_audio(audio: np.ndarray, sr: int = SR) -> Dict[str, Any]:
    """ì˜¤ë””ì˜¤ ë¶„ì„"""
    # ê¸°ë³¸ í†µê³„
    stats = {
        'duration': len(audio) / sr,
        'samples': len(audio),
        'mean': float(np.mean(audio)),
        'std': float(np.std(audio)),
        'max': float(np.max(np.abs(audio))),
        'rms': float(np.sqrt(np.mean(audio ** 2))),
        'zero_crossings': int(np.sum(np.diff(np.sign(audio)) != 0) / 2)
    }
    
    # ìŠ¤í™íŠ¸ëŸ´ íŠ¹ì„±
    _, mag, _ = compute_stft(audio)
    spectral_centroid = librosa.feature.spectral_centroid(S=mag, sr=sr)[0]
    spectral_rolloff = librosa.feature.spectral_rolloff(S=mag, sr=sr)[0]
    
    stats['spectral'] = {
        'centroid_mean': float(np.mean(spectral_centroid)),
        'centroid_std': float(np.std(spectral_centroid)),
        'rolloff_mean': float(np.mean(spectral_rolloff)),
        'rolloff_std': float(np.std(spectral_rolloff))
    }
    
    return stats

def evaluate_separation(original: np.ndarray, separated: List[np.ndarray]) -> Dict[str, float]:
    """ë¶„ë¦¬ í’ˆì§ˆ í‰ê°€"""
    original_energy = np.sum(original ** 2)
    separated_energy = sum(np.sum(s ** 2) for s in separated)
    
    # ì¬êµ¬ì„±
    reconstructed = np.sum(separated, axis=0) if len(separated) > 0 else np.zeros_like(original)
    mse = np.mean((original - reconstructed) ** 2)
    
    # SDR (Signal-to-Distortion Ratio)
    sdr = 10 * np.log10(original_energy / (mse + 1e-8)) if mse > 0 else float('inf')
    
    return {
        'energy_preservation': float(separated_energy / (original_energy + 1e-8)),
        'reconstruction_mse': float(mse),
        'sdr_db': float(sdr),
        'num_sources': len(separated)
    }

# ==================== ë©”ì¸ ì‹¤í–‰ ====================
def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description='AST ê¸°ë°˜ ìŒì› ë¶„ë¦¬ íŒŒì´í”„ë¼ì¸',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì œ:
  python separator.py --input audio.wav
  python separator.py --input audio.wav --output results --no-viz
  python separator.py --input audio.wav --analyze
        """
    )
    
    parser.add_argument('--input', '-i', type=str, required=True,
                       help='ì…ë ¥ ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--output', '-o', type=str, default='output',
                       help='ì¶œë ¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸: output)')
    parser.add_argument('--no-viz', action='store_true',
                       help='ì‹œê°í™” ë¹„í™œì„±í™”')
    parser.add_argument('--analyze', action='store_true',
                       help='ì˜¤ë””ì˜¤ ë¶„ì„ ì •ë³´ ì¶œë ¥')
    
    args = parser.parse_args()
    
    # ì…ë ¥ í™•ì¸
    input_path = Path(args.input)
    if not input_path.exists():
        print(f"ì˜¤ë¥˜: íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_path}")
        sys.exit(1)
    
    # í—¤ë” ì¶œë ¥
    print("\n" + "="*60)
    print("AST ê¸°ë°˜ ìŒì› ë¶„ë¦¬ íŒŒì´í”„ë¼ì¸ v1.0")
    print("="*60)
    print(f"ì„¤ì •:")
    print(f"  - ì…ë ¥: {INPUT_SEC}ì´ˆ")
    print(f"  - ëª¨ë¸: {MODEL_SEC}ì´ˆ")
    print(f"  - ë””ë°”ì´ìŠ¤: CPU")
    print(f"  - ìƒ˜í”Œë§: {SR} Hz")
    print(f"  - STFT: N={N_FFT}, Hop={HOP}")
    print("="*60)
    
    try:
        # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        pipeline = SourceSeparationPipeline(output_dir=args.output)
        
        # ë¶„ì„ ëª¨ë“œ
        if args.analyze:
            print("\nì˜¤ë””ì˜¤ ë¶„ì„ ì¤‘...")
            audio, _ = load_audio(str(input_path))
            analysis = analyze_audio(audio)
            print("\nì˜¤ë””ì˜¤ íŠ¹ì„±:")
            print(f"  - ê¸¸ì´: {analysis['duration']:.3f}ì´ˆ")
            print(f"  - RMS: {analysis['rms']:.4f}")
            print(f"  - ì˜êµì°¨ìœ¨: {analysis['zero_crossings']}")
            print(f"  - ìŠ¤í™íŠ¸ëŸ´ ì¤‘ì‹¬: {analysis['spectral']['centroid_mean']:.1f} Hz")
            print(f"  - ìŠ¤í™íŠ¸ëŸ´ ë¡¤ì˜¤í”„: {analysis['spectral']['rolloff_mean']:.1f} Hz")
        
        # ì²˜ë¦¬
        result = pipeline.process(str(input_path), visualize=not args.no_viz)
        
        # í’ˆì§ˆ í‰ê°€
        if result['num_sources'] > 0:
            print("\në¶„ë¦¬ í’ˆì§ˆ í‰ê°€:")
            audio, _ = load_audio(str(input_path))
            separated = []
            for source in result['sources']:
                sep_audio, _ = librosa.load(source['audio_file'], sr=SR)
                separated.append(sep_audio[:len(audio)])
            
            quality = evaluate_separation(audio, separated)
            print(f"  - ì—ë„ˆì§€ ë³´ì¡´: {quality['energy_preservation']:.2%}")
            print(f"  - SDR: {quality['sdr_db']:.1f} dB")
            print(f"  - MSE: {quality['reconstruction_mse']:.6f}")
        
        # ì™„ë£Œ
        print(f"\nê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {Path(args.output).absolute()}")
        print("\nì²˜ë¦¬ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        
    except KeyboardInterrupt:
        print("\n\nì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        sys.exit(1)
    except Exception as e:
        print(f"\nì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()
