#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
FSD50K ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•œ ìŒì› ë¶„ë¦¬ ì„±ëŠ¥ ì¸¡ì •
"""

import os
import sys
import time
import pandas as pd
import numpy as np
import soundfile as sf
import librosa
import torch
import re
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import torchaudio
from typing import List, Dict, Tuple, Any, Optional
import warnings
warnings.filterwarnings("ignore")

# ì„±ëŠ¥ ì¸¡ì • ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    from mir_eval.separation import bss_eval_sources
    MIR_EVAL_AVAILABLE = True
except ImportError:
    MIR_EVAL_AVAILABLE = False

# separator.py import
sys.path.append(os.path.dirname(__file__))
try:
    import separator
    from transformers import ASTFeatureExtractor, ASTForAudioClassification
    SEPARATOR_AVAILABLE = True
except ImportError:
    SEPARATOR_AVAILABLE = False

class FSD50KEvaluator:
    """FSD50K í‰ê°€ê¸°"""
    
    def __init__(self, 
                 file_selection_threshold: float = 0.7,
                 classification_threshold: float = 0.6,
                 min_confidence: float = 0.1,
                 max_files_per_evaluation: int = 50):
        """
        Args:
            file_selection_threshold: íŒŒì¼ ì„ íƒ ì‹œ ì˜ë¯¸ì  ìœ ì‚¬ë„ ì„ê³„ê°’
            classification_threshold: ë¶„ë¥˜ ë§¤ì¹­ ì‹œ ì˜ë¯¸ì  ìœ ì‚¬ë„ ì„ê³„ê°’
            min_confidence: ìµœì†Œ ì‹ ë¢°ë„ ì„ê³„ê°’
            max_files_per_evaluation: í‰ê°€ë‹¹ ìµœëŒ€ íŒŒì¼ ìˆ˜
        """
        self.file_selection_threshold = file_selection_threshold
        self.classification_threshold = classification_threshold
        self.min_confidence = min_confidence
        self.max_files_per_evaluation = max_files_per_evaluation
        self.eval_audio_dir = "FSD50K.eval_audio/FSD50K.eval_audio"
        self.ground_truth_file = "FSD50K.ground_truth/eval.csv"
        self.results = []
        
        # ë¡œê·¸ íŒŒì¼ ìƒì„±
        self.log_file = open("evaluation_log.txt", "w", encoding="utf-8")
        self.log("FSD50K Performance Evaluation Started")
        
        # Ground truth ë¡œë“œ
        self.ground_truth_df = pd.read_csv(self.ground_truth_file)
        self.log(f"Loaded {len(self.ground_truth_df)} ground truth entries")
        
        # í´ë˜ìŠ¤ ë§¤í•‘ í…Œì´ë¸” ì œê±° (ìë™ ìœ ì‚¬ë„ ê³„ì‚°ìœ¼ë¡œ ëŒ€ì²´)
        
        # Sentence Transformer ëª¨ë¸ ì´ˆê¸°í™” (ìë™ ìœ ì‚¬ë„ ê³„ì‚°ìš©)
        try:
            self.similarity_model = SentenceTransformer('all-MiniLM-L6-v2')
            self.log("âœ… Sentence Transformer loaded for automatic similarity calculation")
        except Exception as e:
            self.similarity_model = None
            self.log(f"âš ï¸ Sentence Transformer failed to load: {e}")
            self.log("   Semantic similarity will be disabled - only exact matches will be used")
        
        # Sound Separator ì´ˆê¸°í™”
        global SEPARATOR_AVAILABLE
        if SEPARATOR_AVAILABLE:
            self.log("Separator module loaded successfully")
        else:
            self.log("Warning: Separator module not available")
    
    def log(self, message: str):
        """ë¡œê·¸ ë©”ì‹œì§€ ì¶œë ¥ ë° íŒŒì¼ ì €ì¥"""
        print(message)
        self.log_file.write(message + "\n")
    
    
    def _normalize_class_name(self, class_name: str) -> str:
        """í´ë˜ìŠ¤ ì´ë¦„ ì •ê·œí™”"""
        # ì†Œë¬¸ì ë³€í™˜ ë° íŠ¹ìˆ˜ë¬¸ì ì œê±°
        normalized = re.sub(r'[^\w\s]', '', class_name.lower())
        # ê³µë°±ì„ ì–¸ë”ìŠ¤ì½”ì–´ë¡œ ë³€í™˜
        normalized = re.sub(r'\s+', '_', normalized)
        return normalized
    
    
    def _calculate_semantic_similarity(self, str1: str, str2: str) -> float:
        """ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚° (Sentence Transformers ì‚¬ìš©)"""
        if self.similarity_model is None:
            # Sentence Transformerê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ë°˜í™˜
            return 0.0
        
        try:
            # ë¬¸ì¥ì„ ë²¡í„°ë¡œ ë³€í™˜
            embeddings = self.similarity_model.encode([str1, str2])
            
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]
            
            return float(similarity)
        except Exception as e:
            self.log(f"âš ï¸ Semantic similarity calculation failed: {e}")
            return 0.0
    
    def _find_best_match(self, predicted_class: str, true_classes: List[str], threshold: float = 0.6) -> Tuple[str, float]:
        """ì˜ˆì¸¡ëœ í´ë˜ìŠ¤ì™€ ì‹¤ì œ í´ë˜ìŠ¤ë“¤ ì¤‘ ê°€ì¥ ìœ ì‚¬í•œ ê²ƒ ì°¾ê¸°"""
        best_match = None
        best_similarity = 0.0
        
        # ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚° (Sentence Transformers)
        for true_class in true_classes:
            similarity = self._calculate_semantic_similarity(predicted_class, true_class)
            if similarity > best_similarity and similarity >= threshold:
                best_similarity = similarity
                best_match = true_class
        
        return best_match, best_similarity
    
    def _find_matching_target_class(self, predicted_class: str, target_classes_set: set) -> Optional[str]:
        """ì˜ˆì¸¡ëœ í´ë˜ìŠ¤ê°€ íƒ€ê²Ÿ í´ë˜ìŠ¤ ì¤‘ í•˜ë‚˜ì™€ ë§¤ì¹­ë˜ëŠ”ì§€ í™•ì¸"""
        if not predicted_class or not target_classes_set:
            return None
        
        # ì •í™•í•œ ë§¤ì¹­ ë¨¼ì € í™•ì¸
        if predicted_class in target_classes_set:
            return predicted_class
        
        # ìœ ì‚¬ë„ ê¸°ë°˜ ë§¤ì¹­
        target_classes_list = list(target_classes_set)
        best_match, similarity = self._find_best_match(predicted_class, target_classes_list)
        
        return best_match if similarity > self.classification_threshold else None
    
    def _map_predicted_to_true_classes(self, predicted_classes: List[str], true_classes: List[str]) -> List[str]:
        """ì˜ˆì¸¡ëœ í´ë˜ìŠ¤ë“¤ì„ ì‹¤ì œ í´ë˜ìŠ¤ë“¤ë¡œ ë§¤í•‘"""
        mapped_classes = []
        used_true_classes = set()
        
        for pred_class in predicted_classes:
            best_match, similarity = self._find_best_match(pred_class, true_classes)
            if best_match and best_match not in used_true_classes:
                mapped_classes.append(best_match)
                used_true_classes.add(best_match)
                self.log(f"  ğŸ¯ Mapped '{pred_class}' â†’ '{best_match}' (similarity: {similarity:.3f})")
        
        return mapped_classes
        self.log_file.flush()
    
    def get_target_files(self, target_classes: List[str], max_per_class: int = 3, include_mixed: bool = True) -> List[Tuple[str, List[str]]]:
        """íƒ€ê²Ÿ í´ë˜ìŠ¤ê°€ í¬í•¨ëœ ì„ì¸ ì†Œë¦¬ íŒŒì¼ ëª©ë¡ ë°˜í™˜ (ì˜ë¯¸ì  ìœ ì‚¬ë„ í™œìš©)"""
        target_files = []
        class_counts = {cls: 0 for cls in target_classes}
        mixed_files = []
        
        for _, row in self.ground_truth_df.iterrows():
            fname = str(row['fname'])
            labels = [label.strip() for label in row['labels'].split(',')]
            
            # ì˜ë¯¸ì  ìœ ì‚¬ë„ë¥¼ í™œìš©í•œ íƒ€ê²Ÿ í´ë˜ìŠ¤ ë§¤ì¹­
            matched_target_labels = []
            for label in labels:
                # 1. ì •í™•í•œ ë§¤ì¹­ ë¨¼ì € í™•ì¸
                if label in target_classes:
                    matched_target_labels.append(label)
                else:
                    # 2. ì˜ë¯¸ì  ìœ ì‚¬ë„ë¡œ ë§¤ì¹­
                    best_match, similarity = self._find_best_match(label, target_classes, threshold=self.file_selection_threshold)
                    if best_match:
                        matched_target_labels.append(best_match)
                        self.log(f"  ğŸ¯ File {fname}: '{label}' â†’ '{best_match}' (similarity: {similarity:.3f})")
            
            if matched_target_labels:
                file_path = os.path.join(self.eval_audio_dir, fname + ".wav")
                if os.path.exists(file_path):
                    # ì„ì¸ ì†Œë¦¬ íŒŒì¼ë§Œ ì²˜ë¦¬ (íƒ€ê²Ÿ í´ë˜ìŠ¤ê°€ í¬í•¨ëœ)
                    if len(matched_target_labels) >= 1:  # íƒ€ê²Ÿ í´ë˜ìŠ¤ê°€ 1ê°œ ì´ìƒ í¬í•¨ëœ ëª¨ë“  íŒŒì¼
                        mixed_files.append((fname, matched_target_labels))
                        # í´ë˜ìŠ¤ë³„ ì¹´ìš´íŠ¸
                        for label in matched_target_labels:
                            if label in class_counts:
                                class_counts[label] += 1
        
        # ì„ì¸ ì†Œë¦¬ íŒŒì¼ì„ ì¶”ê°€ (ìµœëŒ€ 50ê°œ)
        if include_mixed:
            target_files.extend(mixed_files[:self.max_files_per_evaluation])
        
        self.log(f"Selected {len(target_files)} files for evaluation (with semantic similarity matching)")
        self.log(f"  All files contain target classes: {len(target_files)}")
        
        for cls, count in class_counts.items():
            self.log(f"  {cls}: {count} occurrences")
        
        return target_files
    
    def _validate_results(self, results_df: pd.DataFrame):
        """ê²°ê³¼ ê²€ì¦ ë° í’ˆì§ˆ ì²´í¬"""
        if results_df.empty:
            self.log("âš ï¸ No results to validate")
            return
        
        # ê¸°ë³¸ í†µê³„
        total_files = len(results_df)
        avg_accuracy = results_df['accuracy'].mean()
        avg_confidence = results_df['confidence_avg'].mean()
        
        # í’ˆì§ˆ ì²´í¬
        low_confidence_files = len(results_df[results_df['confidence_avg'] < 0.2])
        zero_accuracy_files = len(results_df[results_df['accuracy'] == 0.0])
        high_accuracy_files = len(results_df[results_df['accuracy'] == 1.0])
        
        self.log(f"\nğŸ“‹ ê²°ê³¼ í’ˆì§ˆ ê²€ì¦:")
        self.log(f"  ì´ íŒŒì¼ ìˆ˜: {total_files}")
        self.log(f"  í‰ê·  ì •í™•ë„: {avg_accuracy:.3f}")
        self.log(f"  í‰ê·  ì‹ ë¢°ë„: {avg_confidence:.3f}")
        self.log(f"  ë‚®ì€ ì‹ ë¢°ë„ íŒŒì¼ (<0.2): {low_confidence_files} ({low_confidence_files/total_files*100:.1f}%)")
        self.log(f"  ì •í™•ë„ 0 íŒŒì¼: {zero_accuracy_files} ({zero_accuracy_files/total_files*100:.1f}%)")
        self.log(f"  ì •í™•ë„ 1 íŒŒì¼: {high_accuracy_files} ({high_accuracy_files/total_files*100:.1f}%)")
        
        # ê²½ê³  ì¡°ê±´
        if low_confidence_files > total_files * 0.5:
            self.log(f"  âš ï¸ ê²½ê³ : 50% ì´ìƒì˜ íŒŒì¼ì´ ë‚®ì€ ì‹ ë¢°ë„ë¥¼ ë³´ì…ë‹ˆë‹¤")
        if zero_accuracy_files > total_files * 0.3:
            self.log(f"  âš ï¸ ê²½ê³ : 30% ì´ìƒì˜ íŒŒì¼ì—ì„œ ì •í™•ë„ê°€ 0ì…ë‹ˆë‹¤")
        if avg_accuracy < 0.3:
            self.log(f"  âš ï¸ ê²½ê³ : ì „ì²´ í‰ê·  ì •í™•ë„ê°€ 30% ë¯¸ë§Œì…ë‹ˆë‹¤")
    
    def load_audio(self, file_path: str) -> np.ndarray:
        """ì˜¤ë””ì˜¤ íŒŒì¼ ë¡œë“œ (10ì´ˆ ì´í•˜ë§Œ)"""
        try:
            if SEPARATOR_AVAILABLE:
                # 10ì´ˆ ì´í•˜ ì˜¤ë””ì˜¤ë§Œ ë¡œë“œ
                audio, sr = librosa.load(file_path, sr=separator.SR, duration=10.0)
            else:
                audio, sr = librosa.load(file_path, sr=16000, duration=10.0)
            return audio
        except Exception as e:
            self.log(f"Error loading {file_path}: {e}")
            return None
    
    def create_mixture(self, source_audio: np.ndarray, noise_ratio: float = 0.0) -> np.ndarray:
        """ì›ë³¸ ì˜¤ë””ì˜¤ ì‚¬ìš© (ì´ë¯¸ ì„ì¸ ì†Œë¦¬ì´ë¯€ë¡œ)"""
        # FSD50Kì˜ ì˜¤ë””ì˜¤ëŠ” ì´ë¯¸ ì„ì¸ ì†Œë¦¬ì´ë¯€ë¡œ ì›ë³¸ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        # í•„ìš”ì‹œ ì•½ê°„ì˜ ë…¸ì´ì¦ˆë§Œ ì¶”ê°€
        if noise_ratio > 0:
            noise = np.random.normal(0, 0.01, len(source_audio))
            mixture = source_audio + noise_ratio * noise
        else:
            mixture = source_audio.copy()
        
        # ì •ê·œí™”
        if np.max(np.abs(mixture)) > 0:
            mixture = mixture / np.max(np.abs(mixture)) * 0.8
        
        return mixture
    
    def separate_audio(self, mixture: np.ndarray, target_classes: List[str] = None) -> Tuple[List[np.ndarray], List[Dict[str, Any]]]:
        """ìŒì› ë¶„ë¦¬ ìˆ˜í–‰ (separator.py í•¨ìˆ˜ ê¸°ë°˜)"""
        if not SEPARATOR_AVAILABLE:
            return [mixture], [{"class_name": "unknown", "confidence": 0.0}]
        
        try:
            # separator.pyì˜ í•¨ìˆ˜ë“¤ì„ ì‚¬ìš©í•˜ì—¬ ë¶„ë¦¬ ìˆ˜í–‰
            device = torch.device("cpu")
            
            # Mel filterbank setup
            fbins = separator.N_FFT//2 + 1
            mel_fb_f2m = torchaudio.functional.melscale_fbanks(
                n_freqs=fbins, f_min=0.0, f_max=separator.SR/2, n_mels=separator.N_MELS,
                sample_rate=separator.SR, norm="slaney"
            )
            mel_fb_m2f = mel_fb_f2m.T.contiguous()
            
            # AST model setup
            extractor = ASTFeatureExtractor.from_pretrained("MIT/ast-finetuned-audioset-10-10-0.4593")
            ast_model = ASTForAudioClassification.from_pretrained(
                "MIT/ast-finetuned-audioset-10-10-0.4593",
                attn_implementation="eager"
            ).to(device).eval()
            
            # Apply quantization
            try:
                ast_model = torch.quantization.quantize_dynamic(
                    ast_model,
                    {torch.nn.Linear, torch.nn.Conv1d},
                    dtype=torch.qint8
                )
            except:
                pass
            
            # ë™ì  íŒ¨ìŠ¤ ìˆ˜ ê³„ì‚° (eval.csvì˜ ì‹¤ì œ ë¼ë²¨ ìˆ˜ ê¸°ë°˜)
            # target_classesëŠ” eval.csvì—ì„œ í•´ë‹¹ íŒŒì¼ì— ê¸°ë¡ëœ ëª¨ë“  í´ë˜ìŠ¤ë“¤
            if target_classes:
                # eval.csvì— ê¸°ë¡ëœ í´ë˜ìŠ¤ ìˆ˜ë§Œí¼ íŒ¨ìŠ¤ ì‹¤í–‰
                num_passes = len(target_classes)
            else:
                # Fallback: ì˜¤ë””ì˜¤ ê¸¸ì´ ê¸°ë°˜
                audio_duration = len(mixture) / separator.SR
                num_passes = max(1, int(audio_duration / 2.0))
            
            print(f"ğŸµ Audio duration: {len(mixture) / separator.SR:.2f}s")
            print(f"ğŸ¯ Target classes: {len(target_classes) if target_classes else 'unknown'}")
            print(f"ğŸ”„ Dynamic passes: {num_passes} (based on {'eval.csv label count' if target_classes else 'duration'})")
            print(f"ğŸ“‹ Using {num_passes} passes as determined by eval.csv labels")
            
            # Multi-pass separation
            separated_audios = []
            class_infos = []
            cur = mixture.copy()
            used_mask_prev = None
            prev_anchors = []
            separated_time_regions = []
            previous_anchors = []
            
            # íƒ€ê²Ÿ í´ë˜ìŠ¤ ì¶”ì ì„ ìœ„í•œ ë³€ìˆ˜
            found_target_classes = set()
            target_classes_set = set(target_classes) if target_classes else set()
            
            for pass_idx in range(num_passes):
                print(f"  --- Pass {pass_idx + 1}/{num_passes} ---")
                
                # Single pass separation
                src_amp, res, er, used_mask, info = separator.single_pass(
                    cur, extractor, ast_model, mel_fb_m2f,
                    used_mask_prev, prev_anchors, pass_idx, None, 1.0, 
                    separated_time_regions, previous_anchors, cur
                )
                
                # Silence ê°ì§€ ì‹œ ì¦‰ì‹œ ì „ì²´ íŒŒì¼ ë¶„ë¦¬ ì¢…ë£Œ (ê°•í™”ëœ ê°ì§€)
                if info:
                    class_name = info.get('class_name', '').lower()
                    confidence = info.get('confidence', 0.0)
                    silence_detected = info.get('silence_detected', False)
                    
                    # Silence í‚¤ì›Œë“œ ê°ì§€
                    silence_keywords = ['silence', 'silent', 'quiet', 'no sound', 'mute', 'hush', 'stillness']
                    is_silence = (class_name in silence_keywords or 
                                  'silence' in class_name or 
                                  'quiet' in class_name or
                                  silence_detected or
                                  confidence < 0.05)
                    
                    if is_silence:
                        print(f"    âš ï¸ Silence detected: '{info.get('class_name', 'unknown')}' (confidence: {confidence:.3f}), stopping entire file separation")
                        # Silenceë¡œ ë¶„ë¥˜ëœ ê²½ìš° ê²°ê³¼ì— ì¶”ê°€í•˜ì§€ ì•ŠìŒ
                        break
                
                if src_amp is not None and len(src_amp) > 0:
                    separated_audios.append(src_amp)
                    # í´ë˜ìŠ¤ ì •ë³´ ì¶”ì¶œ
                    if info and 'class_name' in info:
                        predicted_class = info['class_name']
                        class_infos.append({
                            "class_name": predicted_class,
                            "confidence": info.get('confidence', 0.0),
                            "sound_type": info.get('sound_type', 'unknown'),
                            "pass": pass_idx + 1
                        })
                        
                        # íƒ€ê²Ÿ í´ë˜ìŠ¤ì™€ ë§¤ì¹­ í™•ì¸ (ìœ ì‚¬ë„ ê¸°ë°˜)
                        if target_classes_set:
                            matched_target = self._find_matching_target_class(predicted_class, target_classes_set)
                            if matched_target:
                                found_target_classes.add(matched_target)
                                print(f"    âœ… Found target class: {matched_target} (predicted: {predicted_class})")
                            else:
                                print(f"    ğŸ“ Non-target class: {predicted_class}")
                        else:
                            print(f"    Separated: {predicted_class} (confidence: {info.get('confidence', 0.0):.3f})")
                    else:
                        class_infos.append({
                            "class_name": "unknown",
                            "confidence": 0.0,
                            "sound_type": "unknown",
                            "pass": pass_idx + 1
                        })
                    
                    print(f"    Separated: {info.get('class_name', 'unknown')} (confidence: {info.get('confidence', 0.0):.3f})")
                
                # ëª¨ë“  íƒ€ê²Ÿ í´ë˜ìŠ¤ë¥¼ ì°¾ì•˜ìœ¼ë©´ ì¦‰ì‹œ ì¢…ë£Œ
                if target_classes_set and found_target_classes >= target_classes_set:
                    print(f"    ğŸ¯ All target classes found: {list(found_target_classes)}, stopping separation")
                    break
                
                # ë‹¤ìŒ íŒ¨ìŠ¤ë¥¼ ìœ„í•œ ì¤€ë¹„
                cur = res if res is not None and len(res) > 0 else cur
                used_mask_prev = used_mask
                
                # ì¡°ê¸° ì¢…ë£Œ ì¡°ê±´ (ì—ë„ˆì§€ê°€ ë„ˆë¬´ ë‚®ìœ¼ë©´)
                if er < 0.01:  # ì—ë„ˆì§€ ë¹„ìœ¨ì´ 1% ë¯¸ë§Œì´ë©´ ì¢…ë£Œ
                    print(f"    âš ï¸ Low energy ratio ({er:.3f}), stopping separation")
                    break
            
            print(f"  âœ… Total separated sources: {len(separated_audios)}")
            
            if not separated_audios:
                separated_audios = [mixture]
                class_infos = [{"class_name": "unknown", "confidence": 0.0, "sound_type": "unknown", "pass": 1}]
            
            return separated_audios, class_infos
            
        except Exception as e:
            self.log(f"Separation error: {e}")
            return [mixture], [{"class_name": "unknown", "confidence": 0.0, "sound_type": "unknown"}]
    
    def calculate_mixed_class_prediction_accuracy(self, predicted_classes: List[Dict[str, Any]], true_classes: List[str]) -> Dict[str, Any]:
        """ì„ì¸ ì†Œë¦¬ìš© í´ë˜ìŠ¤ ì˜ˆì¸¡ ì •í™•ë„ ê³„ì‚°"""
        if not predicted_classes:
            return {
                'class_accuracy': 0.0,
                'confidence_avg': 0.0,
                'predicted_classes': [],
                'true_classes': true_classes,
                'mixed_accuracy': 0.0,
                'recall': 0.0,
                'precision': 0.0
            }
        
        # ì˜ˆì¸¡ëœ í´ë˜ìŠ¤ë“¤ ì¶”ì¶œ
        predicted_class_names = [info['class_name'] for info in predicted_classes]
        confidences = [info['confidence'] for info in predicted_classes]
        
        # ì •í™•ë„ ê³„ì‚° (true_classesê°€ ì˜ˆì¸¡ëœ í´ë˜ìŠ¤ ì¤‘ì— ìˆëŠ”ì§€)
        correct_predictions = sum(1 for true_class in true_classes if true_class in predicted_class_names)
        class_accuracy = correct_predictions / len(true_classes) if true_classes else 0.0
        
        # ì„ì¸ ì†Œë¦¬ ì •í™•ë„ (ëª¨ë“  true_classesê°€ ì˜ˆì¸¡ë˜ì—ˆëŠ”ì§€)
        mixed_accuracy = 1.0 if all(true_class in predicted_class_names for true_class in true_classes) else 0.0
        
        # Recall: ì‹¤ì œ í´ë˜ìŠ¤ ì¤‘ ì˜ˆì¸¡ëœ ë¹„ìœ¨
        recall = correct_predictions / len(true_classes) if true_classes else 0.0
        
        # Precision: ì˜ˆì¸¡ëœ í´ë˜ìŠ¤ ì¤‘ ì‹¤ì œ í´ë˜ìŠ¤ ë¹„ìœ¨
        precision = correct_predictions / len(predicted_class_names) if predicted_class_names else 0.0
        
        # í‰ê·  ì‹ ë¢°ë„
        confidence_avg = np.mean(confidences) if confidences else 0.0
        
        return {
            'class_accuracy': class_accuracy,
            'confidence_avg': confidence_avg,
            'predicted_classes': predicted_class_names,
            'true_classes': true_classes,
            'mixed_accuracy': mixed_accuracy,
            'recall': recall,
            'precision': precision
        }
    
    def calculate_class_prediction_accuracy(self, predicted_classes: List[Dict[str, Any]], true_class: str) -> Dict[str, float]:
        """í´ë˜ìŠ¤ ì˜ˆì¸¡ ì •í™•ë„ ê³„ì‚°"""
        if not predicted_classes:
            return {
                'class_accuracy': 0.0,
                'confidence_avg': 0.0,
                'predicted_classes': [],
                'true_class': true_class
            }
        
        # ì˜ˆì¸¡ëœ í´ë˜ìŠ¤ë“¤ ì¶”ì¶œ
        predicted_class_names = [info['class_name'] for info in predicted_classes]
        confidences = [info['confidence'] for info in predicted_classes]
        
        # ì •í™•ë„ ê³„ì‚° (true_classê°€ ì˜ˆì¸¡ëœ í´ë˜ìŠ¤ ì¤‘ì— ìˆëŠ”ì§€)
        class_accuracy = 1.0 if true_class in predicted_class_names else 0.0
        
        # í‰ê·  ì‹ ë¢°ë„
        confidence_avg = np.mean(confidences) if confidences else 0.0
        
        return {
            'class_accuracy': class_accuracy,
            'confidence_avg': confidence_avg,
            'predicted_classes': predicted_class_names,
            'true_class': true_class
        }
    
    def calculate_classification_metrics(self, predicted_classes: List[str], true_classes: List[str]) -> Dict[str, float]:
        """í´ë˜ìŠ¤ ë¶„ë¥˜ ì„±ëŠ¥ ì§€í‘œ ê³„ì‚° (ìœ ì‚¬ë„ ê¸°ë°˜ ë§¤ì¹­ í¬í•¨)"""
        if not predicted_classes and not true_classes:
            return {'accuracy': 0.0, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}
        
        # ìœ ì‚¬ë„ ê¸°ë°˜ ë§¤ì¹­ìœ¼ë¡œ ì˜ˆì¸¡ëœ í´ë˜ìŠ¤ë“¤ì„ ì‹¤ì œ í´ë˜ìŠ¤ë¡œ ë§¤í•‘
        mapped_predicted_classes = self._map_predicted_to_true_classes(predicted_classes, true_classes)
        
        # í´ë˜ìŠ¤ ì§‘í•©ìœ¼ë¡œ ë³€í™˜ (ì¤‘ë³µ ì œê±°)
        pred_set = set(mapped_predicted_classes)
        true_set = set(true_classes)
        
        # True Positive: ì˜ˆì¸¡í–ˆê³  ì‹¤ì œë¡œë„ ìˆëŠ” í´ë˜ìŠ¤
        tp = len(pred_set & true_set)
        
        # False Positive: ì˜ˆì¸¡í–ˆì§€ë§Œ ì‹¤ì œë¡œëŠ” ì—†ëŠ” í´ë˜ìŠ¤
        fp = len(pred_set - true_set)
        
        # False Negative: ì˜ˆì¸¡í•˜ì§€ ì•Šì•˜ì§€ë§Œ ì‹¤ì œë¡œëŠ” ìˆëŠ” í´ë˜ìŠ¤
        fn = len(true_set - pred_set)
        
        # Accuracy: ì „ì²´ í´ë˜ìŠ¤ ì¤‘ ë§ê²Œ ì˜ˆì¸¡í•œ ë¹„ìœ¨
        total_classes = len(pred_set | true_set)
        accuracy = tp / total_classes if total_classes > 0 else 0.0
        
        # Precision: ì˜ˆì¸¡í•œ í´ë˜ìŠ¤ ì¤‘ ì‹¤ì œë¡œ ë§ëŠ” ë¹„ìœ¨
        precision = tp / len(pred_set) if len(pred_set) > 0 else 0.0
        
        # Recall: ì‹¤ì œ í´ë˜ìŠ¤ ì¤‘ ì˜ˆì¸¡í•œ ë¹„ìœ¨
        recall = tp / len(true_set) if len(true_set) > 0 else 0.0
        
        # F1-Score: Precisionê³¼ Recallì˜ ì¡°í™”í‰ê· 
        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0
        
        return {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1_score': f1_score,
            'tp': tp,
            'fp': fp,
            'fn': fn,
            'total_predicted': len(pred_set),
            'total_true': len(true_set),
            'mapped_predicted_classes': mapped_predicted_classes
        }
    
    def evaluate_single_file(self, fname: str, true_classes: List[str], target_classes: List[str]) -> Dict[str, Any]:
        """ë‹¨ì¼ íŒŒì¼ í‰ê°€ (ì§€ì •ëœ í´ë˜ìŠ¤ë§Œ ì˜ˆì¸¡ í™•ì¸)"""
        file_path = os.path.join(self.eval_audio_dir, fname + ".wav")
        
        # ì˜¤ë””ì˜¤ ë¡œë“œ
        mixture_audio = self.load_audio(file_path)
        if mixture_audio is None:
            return {
                'accuracy': 0.0, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0,
                'predicted_classes': [], 'true_classes': true_classes,
                'confidence_avg': 0.0, 'num_separated_sources': 0,
                'tp': 0, 'fp': 0, 'fn': 0, 'total_predicted': 0, 'total_true': len(true_classes)
            }
        
        # ìŒì› ë¶„ë¦¬ ìˆ˜í–‰ (íƒ€ê²Ÿ í´ë˜ìŠ¤ ì „ë‹¬)
        separated_sources, class_infos = self.separate_audio(mixture_audio, target_classes)
        
        # ë¶„ë¦¬ëœ ì†ŒìŠ¤ë“¤ì˜ í´ë˜ìŠ¤ ì˜ˆì¸¡ ê²°ê³¼ ìˆ˜ì§‘
        predicted_classes = []
        confidences = []
        
        for class_info in class_infos:
            if class_info['class_name'] != 'unknown' and class_info['confidence'] > self.min_confidence:
                predicted_classes.append(class_info['class_name'])
                confidences.append(class_info['confidence'])
        
        # ë¹ˆ ì˜ˆì¸¡ ê²°ê³¼ ì²˜ë¦¬
        if not predicted_classes:
            self.log(f"  âš ï¸ No valid predictions found for {fname}")
            predicted_classes = ['unknown']
            confidences = [0.0]
        
        # ì§€ì •ëœ í´ë˜ìŠ¤ë§Œ í•„í„°ë§í•˜ì—¬ í‰ê°€
        # true_classesì—ì„œ target_classesì— í¬í•¨ëœ ê²ƒë§Œ ì¶”ì¶œ
        target_true_classes = [cls for cls in true_classes if cls in target_classes]
        
        # í´ë˜ìŠ¤ ë¶„ë¥˜ ì„±ëŠ¥ ì§€í‘œ ê³„ì‚° (ì§€ì •ëœ í´ë˜ìŠ¤ë§Œ)
        classification_metrics = self.calculate_classification_metrics(predicted_classes, target_true_classes)
        
        # ê²°ê³¼ êµ¬ì„±
        result = {
            **classification_metrics,
            'predicted_classes': predicted_classes,
            'mapped_predicted_classes': classification_metrics.get('mapped_predicted_classes', []),
            'true_classes': true_classes,
            'target_true_classes': target_true_classes,
            'confidence_avg': np.mean(confidences) if confidences else 0.0,
            'num_separated_sources': len(separated_sources)
        }
        
        return result
    
    def get_target_files_by_groups(self, danger_classes: List[str], help_classes: List[str], warning_classes: List[str]) -> Dict[str, List[Tuple[str, List[str]]]]:
        """ê·¸ë£¹ë³„ë¡œ íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° (í•œ íŒŒì¼ì´ ì—¬ëŸ¬ ê·¸ë£¹ì— ì†í•  ìˆ˜ ìˆìŒ)"""
        group_files = {
            'danger': [],
            'help': [],
            'warning': []
        }
        
        # ëª¨ë“  íŒŒì¼ì„ ê²€ì‚¬í•˜ì—¬ ê° ê·¸ë£¹ì— ì†í•˜ëŠ” íŒŒì¼ ì°¾ê¸°
        for _, row in self.ground_truth_df.iterrows():
            fname = str(row['fname'])
            labels = [label.strip() for label in row['labels'].split(',')]
            
            # íŒŒì¼ ì¡´ì¬ í™•ì¸
            file_path = os.path.join(self.eval_audio_dir, fname + ".wav")
            if not os.path.exists(file_path):
                continue
            
            # ê° ê·¸ë£¹ë³„ë¡œ í™•ì¸ (ì˜ë¯¸ì  ìœ ì‚¬ë„ í™œìš©)
            for group_name, group_classes in [('danger', danger_classes), ('help', help_classes), ('warning', warning_classes)]:
                # ì˜ë¯¸ì  ìœ ì‚¬ë„ë¥¼ í™œìš©í•œ ë§¤ì¹­
                matched_group_labels = []
                for label in labels:
                    # 1. ì •í™•í•œ ë§¤ì¹­ ë¨¼ì € í™•ì¸
                    if label in group_classes:
                        matched_group_labels.append(label)
                    else:
                        # 2. ì˜ë¯¸ì  ìœ ì‚¬ë„ë¡œ ë§¤ì¹­
                        best_match, similarity = self._find_best_match(label, group_classes, threshold=self.file_selection_threshold)
                        if best_match:
                            matched_group_labels.append(best_match)
                            self.log(f"  ğŸ¯ Group {group_name} - File {fname}: '{label}' â†’ '{best_match}' (similarity: {similarity:.3f})")
                
                # í•´ë‹¹ ê·¸ë£¹ì˜ í´ë˜ìŠ¤ê°€ í•˜ë‚˜ë¼ë„ í¬í•¨ëœ íŒŒì¼ ì„ íƒ
                if matched_group_labels:
                    group_files[group_name].append((fname, matched_group_labels))
        
        # í†µê³„ ì¶œë ¥
        total_files = len(set([fname for group in group_files.values() for fname, _ in group]))
        self.log(f"Found {total_files} unique files for evaluation")
        self.log(f"  - Total CSV records: {len(self.ground_truth_df)}")
        
        for group_name, files in group_files.items():
            self.log(f"  - {group_name.upper()} group: {len(files)} files")
            
            # ê·¸ë£¹ë³„ í´ë˜ìŠ¤ ë¶„í¬ (ì´ë¯¸ ë§¤ì¹­ëœ ë¼ë²¨ë“¤ ì‚¬ìš©)
            class_stats = {}
            for fname, labels in files:
                for label in labels:
                    class_stats[label] = class_stats.get(label, 0) + 1
            
            self.log(f"    Class distribution:")
            for class_name, count in sorted(class_stats.items()):
                self.log(f"      {class_name}: {count} files")
        
        return group_files
    
    def run_group_evaluation(self, danger_classes: List[str], help_classes: List[str], warning_classes: List[str]):
        """ê·¸ë£¹ë³„ í‰ê°€ ì‹¤í–‰ (í•œ íŒŒì¼ì´ ì—¬ëŸ¬ ê·¸ë£¹ì— ì†í•  ìˆ˜ ìˆìŒ)"""
        self.log("Starting FSD50K group-based classification evaluation...")
        
        # ê·¸ë£¹ë³„ íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        group_files = self.get_target_files_by_groups(danger_classes, help_classes, warning_classes)
        
        all_results = []
        
        # ê° ê·¸ë£¹ë³„ë¡œ í‰ê°€ ìˆ˜í–‰
        for group_name, files in group_files.items():
            if not files:
                self.log(f"No files found for {group_name} group")
                continue
                
            group_classes = danger_classes if group_name == 'danger' else help_classes if group_name == 'help' else warning_classes
            self.log(f"\n=== Evaluating {group_name.upper()} group ({len(files)} files) ===")
            
            # ê·¸ë£¹ ë‚´ íŒŒì¼ë“¤ í‰ê°€
            for i, (fname, all_labels) in enumerate(files):
                file_start_time = time.time()
                self.log(f"Processing {i+1}/{len(files)}: {fname}")
                self.log(f"  ğŸ“‹ All labels in file: {all_labels}")
                
                # í•´ë‹¹ ê·¸ë£¹ì˜ í´ë˜ìŠ¤ë§Œ í•„í„°ë§
                group_labels = [label for label in all_labels if label in group_classes]
                self.log(f"  ğŸ¯ Target {group_name} group labels: {group_labels}")
                self.log(f"  ğŸ” Will separate ALL sounds and check if {group_name} classes are detected")
                
                if not group_labels:
                    self.log(f"  âš ï¸ No {group_name} group labels found, skipping")
                    continue
                
                # ë‹¨ì¼ íŒŒì¼ í‰ê°€ (í•´ë‹¹ ê·¸ë£¹ í´ë˜ìŠ¤ë§Œ)
                self.log(f"  ğŸµ Starting separation for {len(group_labels)} target classes...")
                metrics = self.evaluate_single_file(fname, all_labels, group_classes)
                
                result = {
                    'group': group_name,
                    'filename': fname,
                    'all_labels': ', '.join(all_labels),
                    'group_labels': ', '.join(group_labels),
                    'accuracy': metrics['accuracy'],
                    'precision': metrics['precision'],
                    'recall': metrics['recall'],
                    'f1_score': metrics['f1_score'],
                    'confidence_avg': metrics['confidence_avg'],
                    'predicted_classes': ', '.join(metrics['predicted_classes']),
                    'mapped_predicted_classes': ', '.join(metrics.get('mapped_predicted_classes', [])),
                    'num_separated_sources': metrics['num_separated_sources'],
                    'tp': metrics['tp'],
                    'fp': metrics['fp'],
                    'fn': metrics['fn'],
                    'total_predicted': metrics['total_predicted'],
                    'total_true': metrics['total_true']
                }
                
                all_results.append(result)
                
                file_elapsed_time = time.time() - file_start_time
                self.log(f"  ğŸ“Š Results:")
                self.log(f"    Accuracy: {metrics['accuracy']:.3f}, Precision: {metrics['precision']:.3f}, Recall: {metrics['recall']:.3f}, F1: {metrics['f1_score']:.3f}")
                self.log(f"    Confidence: {metrics['confidence_avg']:.3f}, Separated Sources: {metrics['num_separated_sources']}")
                self.log(f"    ğŸ¯ Predicted classes: {', '.join(metrics['predicted_classes'])}")
                self.log(f"    âœ… Mapped to target: {', '.join(metrics.get('mapped_predicted_classes', []))}")
                self.log(f"    ğŸ¯ Target classes found: {len(metrics.get('mapped_predicted_classes', []))}/{len(group_labels)}")
                self.log(f"    â±ï¸ Processing time: {file_elapsed_time:.2f}s")
                
                if (i + 1) % 10 == 0:
                    progress_pct = (i + 1) / len(files) * 100
                    self.log(f"  Completed {i+1}/{len(files)} files in {group_name} group ({progress_pct:.1f}%)")
                    
                    # í˜„ì¬ê¹Œì§€ì˜ í‰ê·  ì„±ëŠ¥ í‘œì‹œ
                    if i > 0:
                        recent_results = [r for r in self.results[-10:]]
                        if recent_results:
                            avg_accuracy = np.mean([r['accuracy'] for r in recent_results])
                            avg_confidence = np.mean([r['confidence_avg'] for r in recent_results])
                            self.log(f"    Recent 10 files - Avg Accuracy: {avg_accuracy:.3f}, Avg Confidence: {avg_confidence:.3f}")
        
        # ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
        results_df = pd.DataFrame(all_results)
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del all_results
        import gc
        gc.collect()
        
        # ì „ì²´ ì„±ëŠ¥ì§€í‘œ ì½˜ì†” ì¶œë ¥
        if not results_df.empty:
            self.log("\n" + "="*60)
            self.log("ğŸ“Š ì „ì²´ ì„±ëŠ¥ì§€í‘œ ìš”ì•½")
            self.log("="*60)
            self.log(f"  ì´ í‰ê°€ íŒŒì¼ ìˆ˜: {len(results_df)}ê°œ")
            self.log(f"  í‰ê·  Accuracy: {results_df['accuracy'].mean():.3f} Â± {results_df['accuracy'].std():.3f}")
            self.log(f"  í‰ê·  Precision: {results_df['precision'].mean():.3f} Â± {results_df['precision'].std():.3f}")
            self.log(f"  í‰ê·  Recall: {results_df['recall'].mean():.3f} Â± {results_df['recall'].std():.3f}")
            self.log(f"  í‰ê·  F1-Score: {results_df['f1_score'].mean():.3f} Â± {results_df['f1_score'].std():.3f}")
            self.log(f"  í‰ê·  Confidence: {results_df['confidence_avg'].mean():.3f} Â± {results_df['confidence_avg'].std():.3f}")
            self.log(f"  í‰ê·  ë¶„ë¦¬ ì†ŒìŠ¤ ìˆ˜: {results_df['num_separated_sources'].mean():.1f} Â± {results_df['num_separated_sources'].std():.1f}")
            
            # ê·¸ë£¹ë³„ ì„±ëŠ¥ì§€í‘œ ìš”ì•½
            self.log("\nğŸ“ˆ ê·¸ë£¹ë³„ ì„±ëŠ¥ì§€í‘œ:")
            for group in ['danger', 'help', 'warning']:
                group_data = results_df[results_df['group'] == group]
                if not group_data.empty:
                    self.log(f"  ğŸ”´ {group.upper()} ê·¸ë£¹ ({len(group_data)}ê°œ íŒŒì¼):")
                    self.log(f"    Accuracy: {group_data['accuracy'].mean():.3f} Â± {group_data['accuracy'].std():.3f}")
                    self.log(f"    F1-Score: {group_data['f1_score'].mean():.3f} Â± {group_data['f1_score'].std():.3f}")
                    self.log(f"    Confidence: {group_data['confidence_avg'].mean():.3f} Â± {group_data['confidence_avg'].std():.3f}")
        
        # ê²°ê³¼ ê²€ì¦ ë° í’ˆì§ˆ ì²´í¬
        self._validate_results(results_df)
        
        # ê²°ê³¼ ì €ì¥
        self.save_group_results(results_df)
        
        return results_df
    
    def run_evaluation(self, target_classes: List[str], max_per_class: int = 3):
        """ì „ì²´ í‰ê°€ ì‹¤í–‰ (í´ë˜ìŠ¤ ë¶„ë¥˜)"""
        self.log("Starting FSD50K classification evaluation...")
        
        target_files = self.get_target_files(target_classes, max_per_class)
        
        for i, (fname, true_classes) in enumerate(target_files):
            class_str = ', '.join(true_classes) if len(true_classes) > 1 else true_classes[0]
            self.log(f"Processing {i+1}/{len(target_files)}: {fname} ({class_str})")
            
            # ì§€ì •ëœ í´ë˜ìŠ¤ë§Œ í•„í„°ë§
            target_true_classes = [cls for cls in true_classes if cls in target_classes]
            self.log(f"  Target classes in file: {target_true_classes}")
            
            metrics = self.evaluate_single_file(fname, true_classes, target_classes)
            
            result = {
                'filename': fname,
                'true_classes': class_str,
                'target_classes': ', '.join(target_true_classes),
                'accuracy': metrics['accuracy'],
                'precision': metrics['precision'],
                'recall': metrics['recall'],
                'f1_score': metrics['f1_score'],
                'confidence_avg': metrics['confidence_avg'],
                'predicted_classes': ', '.join(metrics['predicted_classes']),
                'mapped_predicted_classes': ', '.join(metrics.get('mapped_predicted_classes', [])),
                'num_separated_sources': metrics['num_separated_sources'],
                'tp': metrics['tp'],
                'fp': metrics['fp'],
                'fn': metrics['fn'],
                'total_predicted': metrics['total_predicted'],
                'total_true': metrics['total_true']
            }
            
            self.results.append(result)
            self.log(f"  Accuracy: {metrics['accuracy']:.3f}, Precision: {metrics['precision']:.3f}, Recall: {metrics['recall']:.3f}, F1: {metrics['f1_score']:.3f}")
            self.log(f"  Confidence: {metrics['confidence_avg']:.3f}, Sources: {metrics['num_separated_sources']}")
            self.log(f"  Original Predicted: {', '.join(metrics['predicted_classes'])}")
            self.log(f"  Mapped Predicted: {', '.join(metrics.get('mapped_predicted_classes', []))}")
            self.log(f"  True Classes: {', '.join(true_classes)}")
        
        # ê²°ê³¼ ì €ì¥
        results_df = pd.DataFrame(self.results)
        self.save_results(results_df)
        
        return results_df
    
    def save_group_results(self, results_df: pd.DataFrame):
        """ê·¸ë£¹ë³„ ê²°ê³¼ ì €ì¥"""
        if results_df.empty:
            self.log("No results to save")
            return
        
        # CSV íŒŒì¼ë¡œ ì €ì¥
        csv_filename = "fsd50k_group_performance_results.csv"
        results_df.to_csv(csv_filename, index=False)
        self.log(f"Results saved to {csv_filename}")
        
        # ê·¸ë£¹ë³„ ìš”ì•½ í†µê³„
        summary_filename = "fsd50k_group_evaluation_summary.txt"
        with open(summary_filename, "w", encoding="utf-8") as f:
            f.write("FSD50K Group-Based Classification Performance Summary\n")
            f.write("=" * 60 + "\n\n")
            
            # ì „ì²´ í†µê³„
            f.write("Overall Performance:\n")
            f.write(f"  Total files evaluated: {len(results_df)}\n")
            f.write(f"  Average accuracy: {results_df['accuracy'].mean():.3f}\n")
            f.write(f"  Average precision: {results_df['precision'].mean():.3f}\n")
            f.write(f"  Average recall: {results_df['recall'].mean():.3f}\n")
            f.write(f"  Average F1-score: {results_df['f1_score'].mean():.3f}\n")
            f.write(f"  Average confidence: {results_df['confidence_avg'].mean():.3f}\n")
            f.write(f"  Average separated sources: {results_df['num_separated_sources'].mean():.1f}\n\n")
            
            # ê·¸ë£¹ë³„ í†µê³„
            for group in ['danger', 'help', 'warning']:
                group_data = results_df[results_df['group'] == group]
                if not group_data.empty:
                    f.write(f"{group.upper()} Group Performance:\n")
                    f.write(f"  Files evaluated: {len(group_data)}\n")
                    f.write(f"  Average accuracy: {group_data['accuracy'].mean():.3f}\n")
                    f.write(f"  Average precision: {group_data['precision'].mean():.3f}\n")
                    f.write(f"  Average recall: {group_data['recall'].mean():.3f}\n")
                    f.write(f"  Average F1-score: {group_data['f1_score'].mean():.3f}\n")
                    f.write(f"  Average confidence: {group_data['confidence_avg'].mean():.3f}\n")
                    f.write(f"  Average separated sources: {group_data['num_separated_sources'].mean():.1f}\n\n")
        
        self.log(f"Summary saved to {summary_filename}")
    
    def save_results(self, results_df: pd.DataFrame):
        """ê²°ê³¼ ì €ì¥"""
        # CSV íŒŒì¼ë¡œ ì €ì¥
        output_file = "fsd50k_performance_results.csv"
        results_df.to_csv(output_file, index=False)
        self.log(f"Results saved to: {output_file}")
        
        # ìš”ì•½ ê²°ê³¼ ì €ì¥
        summary_file = "evaluation_summary.txt"
        with open(summary_file, "w", encoding="utf-8") as f:
            f.write("FSD50K CLASSIFICATION EVALUATION RESULTS\n")
            f.write("="*50 + "\n\n")
            
            if len(results_df) > 0:
                # ì „ì²´ í‰ê· 
                f.write("Overall Average:\n")
                f.write(f"  Accuracy: {results_df['accuracy'].mean():.3f} Â± {results_df['accuracy'].std():.3f}\n")
                f.write(f"  Precision: {results_df['precision'].mean():.3f} Â± {results_df['precision'].std():.3f}\n")
                f.write(f"  Recall: {results_df['recall'].mean():.3f} Â± {results_df['recall'].std():.3f}\n")
                f.write(f"  F1-Score: {results_df['f1_score'].mean():.3f} Â± {results_df['f1_score'].std():.3f}\n")
                f.write(f"  Confidence: {results_df['confidence_avg'].mean():.3f} Â± {results_df['confidence_avg'].std():.3f}\n")
                f.write(f"  Avg Sources: {results_df['num_separated_sources'].mean():.1f} Â± {results_df['num_separated_sources'].std():.1f}\n\n")
                
                # í´ë˜ìŠ¤ë³„ í‰ê· 
                f.write("Per-Class Average:\n")
                class_avg = results_df.groupby('true_classes').agg({
                    'accuracy': ['mean', 'std'],
                    'precision': ['mean', 'std'],
                    'recall': ['mean', 'std'],
                    'f1_score': ['mean', 'std'],
                    'confidence_avg': ['mean', 'std'],
                    'num_separated_sources': ['mean', 'std']
                }).round(3)
                
                for class_name in class_avg.index:
                    f.write(f"\n  {class_name}:\n")
                    f.write(f"    Accuracy: {class_avg.loc[class_name, ('accuracy', 'mean')]:.3f} Â± {class_avg.loc[class_name, ('accuracy', 'std')]:.3f}\n")
                    f.write(f"    Precision: {class_avg.loc[class_name, ('precision', 'mean')]:.3f} Â± {class_avg.loc[class_name, ('precision', 'std')]:.3f}\n")
                    f.write(f"    Recall: {class_avg.loc[class_name, ('recall', 'mean')]:.3f} Â± {class_avg.loc[class_name, ('recall', 'std')]:.3f}\n")
                    f.write(f"    F1-Score: {class_avg.loc[class_name, ('f1_score', 'mean')]:.3f} Â± {class_avg.loc[class_name, ('f1_score', 'std')]:.3f}\n")
                    f.write(f"    Confidence: {class_avg.loc[class_name, ('confidence_avg', 'mean')]:.3f} Â± {class_avg.loc[class_name, ('confidence_avg', 'std')]:.3f}\n")
                    f.write(f"    Avg Sources: {class_avg.loc[class_name, ('num_separated_sources', 'mean')]:.1f} Â± {class_avg.loc[class_name, ('num_separated_sources', 'std')]:.1f}\n")
                
                # ìƒì„¸ ê²°ê³¼
                f.write("\nDetailed Results:\n")
                f.write(results_df.to_string(index=False, float_format='%.3f'))
        
        self.log(f"Summary saved to: {summary_file}")
    
    def __del__(self):
        """ì†Œë©¸ì"""
        if hasattr(self, 'log_file'):
            self.log_file.close()


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # ê·¸ë£¹ë³„ í‰ê°€í•  í´ë˜ìŠ¤ ì„ íƒ
    danger_classes = [
        'Siren', 'Civil defense siren', 'Buzzer', 'Smoke detector, smoke alarm', 
        'Fire alarm', 'Explosion', 'Boom'
    ]
    
    help_classes = [
        'Baby cry, infant cry', 'Screaming', 'Door', 'Doorbell', 'Ding-dong', 'Knock'
    ]
    
    warning_classes = [
        'Water', 'Dishes, pots, and pans', 'Alarm', 'Telephone', 'Telephone bell ringing',
        'Splinter', 'Ringtone', 'Telephone dialing, DTMF', 'Dial tone', 'Alarm clock',
        'Crack', 'Glass', 'Chink, clink', 'Shatter', 'Boiling', 'Smash, crash', 'Breaking',
        'Crushing', 'Crumpling, crinkling'
    ]
    
    # ëª¨ë“  í´ë˜ìŠ¤ í•©ì¹˜ê¸°
    target_classes = danger_classes + help_classes + warning_classes
    
    # í‰ê°€ê¸° ì´ˆê¸°í™”
    evaluator = FSD50KEvaluator()
    
    try:
        # ê·¸ë£¹ë³„ í‰ê°€ ì‹¤í–‰ (í•œ íŒŒì¼ì´ ì—¬ëŸ¬ ê·¸ë£¹ì— ì†í•  ìˆ˜ ìˆìŒ)
        results_df = evaluator.run_group_evaluation(
            danger_classes=danger_classes,
            help_classes=help_classes,
            warning_classes=warning_classes
        )
        
        evaluator.log("Group-based evaluation completed successfully!")
        
    except Exception as e:
        evaluator.log(f"Evaluation failed: {e}")
        import traceback
        evaluator.log(traceback.format_exc())


if __name__ == "__main__":
    main()